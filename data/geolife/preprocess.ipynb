{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 0\n",
      "704\n",
      "user_id: 1\n",
      "1204\n",
      "user_id: 3\n",
      "3431\n",
      "user_id: 4\n",
      "5250\n",
      "user_id: 5\n",
      "5592\n",
      "user_id: 6\n",
      "5818\n",
      "user_id: 7\n",
      "6307\n",
      "user_id: 8\n",
      "6462\n",
      "user_id: 9\n",
      "6608\n",
      "user_id: 11\n",
      "6730\n",
      "user_id: 13\n",
      "7182\n",
      "user_id: 14\n",
      "7387\n",
      "user_id: 15\n",
      "7658\n",
      "user_id: 16\n",
      "8016\n",
      "user_id: 18\n",
      "8036\n",
      "user_id: 19\n",
      "8065\n",
      "user_id: 21\n",
      "8065\n",
      "user_id: 24\n",
      "10642\n",
      "user_id: 26\n",
      "11113\n",
      "user_id: 27\n",
      "11183\n",
      "user_id: 29\n",
      "11368\n",
      "user_id: 30\n",
      "12431\n",
      "user_id: 31\n",
      "12504\n",
      "user_id: 32\n",
      "12627\n",
      "user_id: 33\n",
      "12957\n",
      "user_id: 34\n",
      "13072\n",
      "user_id: 35\n",
      "16335\n",
      "user_id: 36\n",
      "17021\n",
      "user_id: 37\n",
      "17315\n",
      "user_id: 38\n",
      "17659\n",
      "user_id: 39\n",
      "19329\n",
      "user_id: 40\n",
      "19528\n",
      "user_id: 43\n",
      "19939\n",
      "user_id: 44\n",
      "20308\n",
      "user_id: 45\n",
      "20369\n",
      "user_id: 46\n",
      "20517\n",
      "user_id: 47\n",
      "20517\n",
      "user_id: 48\n",
      "20602\n",
      "user_id: 49\n",
      "20634\n",
      "user_id: 50\n",
      "21020\n",
      "user_id: 51\n",
      "21077\n",
      "user_id: 54\n",
      "21082\n",
      "user_id: 55\n",
      "21233\n",
      "user_id: 56\n",
      "21238\n",
      "user_id: 57\n",
      "21238\n",
      "user_id: 58\n",
      "21280\n",
      "user_id: 59\n",
      "21322\n",
      "user_id: 61\n",
      "21322\n",
      "user_id: 63\n",
      "21366\n",
      "user_id: 64\n",
      "21409\n",
      "user_id: 65\n",
      "21594\n",
      "user_id: 66\n",
      "21843\n",
      "user_id: 67\n",
      "22551\n",
      "user_id: 68\n",
      "23556\n",
      "user_id: 69\n",
      "23562\n",
      "user_id: 70\n",
      "23617\n",
      "user_id: 73\n",
      "23640\n",
      "user_id: 74\n",
      "23830\n",
      "user_id: 75\n",
      "23981\n",
      "user_id: 76\n",
      "23981\n",
      "user_id: 77\n",
      "23981\n",
      "user_id: 78\n",
      "24090\n",
      "user_id: 79\n",
      "24180\n",
      "user_id: 80\n",
      "24180\n",
      "user_id: 81\n",
      "24213\n",
      "user_id: 85\n",
      "25229\n",
      "user_id: 86\n",
      "25229\n",
      "user_id: 88\n",
      "25272\n",
      "user_id: 93\n",
      "25282\n",
      "user_id: 94\n",
      "25282\n",
      "user_id: 95\n",
      "25397\n",
      "user_id: 96\n",
      "25599\n",
      "user_id: 97\n",
      "25599\n",
      "user_id: 98\n",
      "25599\n",
      "user_id: 100\n",
      "25599\n",
      "user_id: 101\n",
      "25600\n",
      "user_id: 102\n",
      "25619\n",
      "user_id: 103\n",
      "25674\n",
      "user_id: 104\n",
      "25679\n",
      "user_id: 105\n",
      "25679\n",
      "user_id: 106\n",
      "25679\n",
      "user_id: 109\n",
      "25679\n",
      "user_id: 110\n",
      "25683\n",
      "user_id: 112\n",
      "25743\n",
      "user_id: 113\n",
      "26012\n",
      "user_id: 114\n",
      "26078\n",
      "user_id: 116\n",
      "26078\n",
      "user_id: 119\n",
      "26441\n",
      "user_id: 121\n",
      "26452\n",
      "user_id: 122\n",
      "26619\n",
      "user_id: 125\n",
      "27037\n",
      "user_id: 129\n",
      "27043\n",
      "user_id: 130\n",
      "27122\n",
      "user_id: 131\n",
      "27161\n",
      "user_id: 133\n",
      "27167\n",
      "user_id: 134\n",
      "27167\n",
      "user_id: 135\n",
      "27438\n",
      "user_id: 136\n",
      "27438\n",
      "user_id: 137\n",
      "27606\n",
      "user_id: 138\n",
      "27607\n",
      "user_id: 141\n",
      "27617\n",
      "user_id: 143\n",
      "27630\n",
      "user_id: 145\n",
      "27699\n",
      "user_id: 147\n",
      "27807\n",
      "user_id: 148\n",
      "27908\n",
      "user_id: 149\n",
      "27951\n",
      "user_id: 150\n",
      "27951\n",
      "user_id: 151\n",
      "27958\n",
      "user_id: 152\n",
      "27961\n",
      "user_id: 154\n",
      "27964\n",
      "user_id: 155\n",
      "28018\n",
      "user_id: 156\n",
      "28063\n",
      "user_id: 157\n",
      "28063\n",
      "user_id: 158\n",
      "28082\n",
      "user_id: 159\n",
      "28287\n",
      "user_id: 161\n",
      "28288\n",
      "user_id: 165\n",
      "28300\n",
      "user_id: 166\n",
      "28300\n",
      "user_id: 169\n",
      "28433\n",
      "user_id: 170\n",
      "28433\n",
      "user_id: 173\n",
      "28433\n",
      "user_id: 174\n",
      "28439\n",
      "user_id: 177\n",
      "28456\n",
      "user_id: 179\n",
      "28539\n",
      "user_id: 180\n",
      "28727\n",
      "user_id: 181\n",
      "28727\n"
     ]
    }
   ],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "\n",
    "days_min = 0.000696\n",
    "gap = 30\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "\n",
    "round_time = round_sec\n",
    "\n",
    "segment_csv = '_segment_time_grid_' + round_time + '.csv'\n",
    "output_csv = '_segment_output_' + round_time + '.csv'\n",
    "\n",
    "train_csv =  '_train_' + round_time + '.csv'\n",
    "valid_csv =  '_valid_' + round_time + '.csv'\n",
    "test_csv =  '_test_' + round_time + '.csv'\n",
    "\n",
    "train_ratio = 0.7\n",
    "\n",
    "user_id_list = []\n",
    "time_grid_list = []\n",
    "ratio = []\n",
    "\n",
    "min = 2\n",
    "min_length = 20 * min\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "output_file = './Data/' + output_csv\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    # full_df = pd.DataFrame()\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "    # output_file = './Data/' + user_id + '/csv/' + user_id + output_csv\n",
    "    df = pd.read_csv(segment_file)\n",
    "    \n",
    "    group_df = df.groupby(['group'])['mask'].sum().reset_index()\n",
    "    group_df = group_df.loc[group_df['mask'] >= min_length, :].reset_index(drop=True)\n",
    "    \n",
    "    divide = 5*min\n",
    "    for idx in range(group_df.shape[0]):\n",
    "        group_id = group_df.iloc[idx, 0]\n",
    "        df_temp = df.loc[df['group'] == group_id, :].reset_index(drop=True)\n",
    "        for temp_idx in range(df_temp.shape[0]//divide):\n",
    "            cur_idx = temp_idx * divide\n",
    "            if cur_idx + min_length < df_temp.shape[0]:\n",
    "                cur_df = df_temp.iloc[cur_idx:cur_idx + min_length, :].reset_index(drop=True)\n",
    "                full_df = pd.concat([full_df, cur_df], axis = 0).reset_index(drop=True)\n",
    "    print(full_df.shape[0]//40)\n",
    "full_df.to_csv(output_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T06:00:40.516535Z",
     "start_time": "2024-03-28T05:49:54.178955Z"
    }
   },
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:52:12.543719Z",
     "start_time": "2024-03-28T03:51:28.533180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 6\n",
      "user_id: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 18\n",
      "user_id: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 21\n",
      "user_id: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 27\n",
      "user_id: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 31\n",
      "user_id: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 45\n",
      "user_id: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 47\n",
      "user_id: 48\n",
      "user_id: 49\n",
      "user_id: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 51\n",
      "user_id: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 55\n",
      "user_id: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 57\n",
      "user_id: 58\n",
      "user_id: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 61\n",
      "user_id: 63\n",
      "user_id: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 69\n",
      "user_id: 70\n",
      "user_id: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 75\n",
      "user_id: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 77\n",
      "user_id: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 79\n",
      "user_id: 80\n",
      "user_id: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 86\n",
      "user_id: 88\n",
      "user_id: 93\n",
      "user_id: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 95\n",
      "user_id: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 97\n",
      "user_id: 98\n",
      "user_id: 100\n",
      "user_id: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 102\n",
      "user_id: 103\n",
      "user_id: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 105\n",
      "user_id: 106\n",
      "user_id: 109\n",
      "user_id: 110\n",
      "user_id: 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 113\n",
      "user_id: 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 116\n",
      "user_id: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 129\n",
      "user_id: 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 131\n",
      "user_id: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 134\n",
      "user_id: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 136\n",
      "user_id: 137\n",
      "user_id: 138\n",
      "user_id: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 143\n",
      "user_id: 145\n",
      "user_id: 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 148\n",
      "user_id: 149\n",
      "user_id: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 151\n",
      "user_id: 152\n",
      "user_id: 154\n",
      "user_id: 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 156\n",
      "user_id: 157\n",
      "user_id: 158\n",
      "user_id: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 161\n",
      "user_id: 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 166\n",
      "user_id: 169\n",
      "user_id: 170\n",
      "user_id: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 174\n",
      "user_id: 177\n",
      "user_id: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 180\n",
      "user_id: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
      "/tmp/ipykernel_520391/1424431415.py:69: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# csv  5   (rounded)\n",
    "# round_min   round_min  data \n",
    "\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "\n",
    "days_min = 0.000696\n",
    "gap = 30\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "\n",
    "round_time = round_sec\n",
    "\n",
    "time_delta = 20\n",
    "segment_delta = str(time_delta) + 'min'\n",
    "\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "\n",
    "grid_csv = '_origin_grid_' + round_time + '.csv'\n",
    "segment_csv = '_segment_time_grid_' + round_time + '.csv'\n",
    "\n",
    "train_csv =  '_train_' + round_time + '.csv'\n",
    "valid_csv =  '_valid_' + round_time + '.csv'\n",
    "test_csv =  '_test_' + round_time + '.csv'\n",
    "\n",
    "train_ratio = 0.7\n",
    "\n",
    "user_id_list = []\n",
    "begin_list = []\n",
    "end_list = []\n",
    "time_grid_list = []\n",
    "ratio = []\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    # id = 3\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    grid_file = './Data/' + user_id + '/csv/' + user_id + grid_csv\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "\n",
    "    train_file = './Data/' + user_id + '/csv/' + user_id + train_csv\n",
    "    valid_file = './Data/' + user_id + '/csv/' + user_id + valid_csv\n",
    "    test_file = './Data/' + user_id + '/csv/' + user_id + test_csv\n",
    "\n",
    "    user_id_list += [user_id]\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + \" \" + df['time'])\n",
    "    df['datetime'] = df['datetime'].dt.round(round_time)\n",
    "\n",
    "    df = df.set_index('datetime').reset_index()\n",
    "    user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
    "    user_df = locationPreprocessor.convert_coord_for_blender_for_user(user_df)\n",
    "\n",
    "    user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "    user_df['prev_date'] = user_df['datetime'].shift(1)\n",
    "    user_df['time_diff'] = user_df['datetime'] - user_df['prev_date']\n",
    "    user_df['time_diff'] = user_df['time_diff'].astype(int)\n",
    "    \n",
    "    group_number = 1\n",
    "    group_column = []\n",
    "    \n",
    "    for idx in range(user_df.shape[0]):\n",
    "        if idx == 0:\n",
    "            group_column.append(group_number)\n",
    "            continue\n",
    "\n",
    "        if user_df.iloc[idx, -1] != 30000000000:\n",
    "            group_number += 1\n",
    "        group_column.append(group_number)\n",
    "\n",
    "    user_df['group'] = group_column\n",
    "\n",
    "    user_df['year'] = user_df['datetime'].dt.year\n",
    "    user_df['month'] = user_df['datetime'].dt.month\n",
    "    user_df['week'] = user_df['datetime'].dt.weekday\n",
    "    user_df['week'] += 1\n",
    "    # user_df['weekend'] = np.where(user_df['week'] < 5, 0, 1)\n",
    "    user_df['hour'] = user_df['datetime'].dt.hour\n",
    "    user_df['hour'] += 1\n",
    "    user_df['day'] = user_df['datetime'].dt.day\n",
    "    user_df['day'] += 1\n",
    "\n",
    "    user_df = user_df.drop(columns=['days', 'datetime', 'latitude', 'longitude', 'what', 'altitude', 'time_diff', 'prev_date'])\n",
    "\n",
    "    # ## make data fully\n",
    "    # begin = user_df['datetime'][0]\n",
    "    # end = user_df['datetime'][user_df.shape[0]-1]\n",
    "    # \n",
    "    # begin_list += [begin]\n",
    "    # end_list += [end]\n",
    "    # print(f'begin: {begin}')\n",
    "    # print(f'end: {end}')\n",
    "    # \n",
    "    # date_df = pd.DataFrame({'datetime':pd.date_range(begin, end, freq=round_time)})\n",
    "    # \n",
    "    # origin_len = user_df.shape[0]\n",
    "    # user_df = pd.merge(date_df, user_df, how='outer', on='datetime')\n",
    "    # ratio += [int(round(origin_len/user_df.shape[0], 2) *  100)]\n",
    "    # user_df = user_df.fillna(0)\n",
    "    # # user_df['time_grid'] = np.arange(1, user_df.shape[0] + 1)\n",
    "    user_df['mask'] = np.where(user_df['x'] != 0, 1, 0)\n",
    "    # \n",
    "    # time_grid_list += [user_df.shape[0]]\n",
    "    # \n",
    "    # # save grid file\n",
    "    # user_df = user_df.drop(columns=['days', 'datetime', 'latitude', 'longitude', 'what', 'altitude'])\n",
    "    # \n",
    "    # # Train-test set\n",
    "    # train_size = int(user_df.shape[0] * train_ratio)\n",
    "    # train_df = user_df.iloc[:train_size,:]\n",
    "    # test_df = user_df.loc[train_size:, :]\n",
    "    # print(f\"train_valid: {train_df.shape}\")\n",
    "    # print(f\"test : {test_df.shape}\")\n",
    "\n",
    "    # train_df.to_csv(train_file, index=False)\n",
    "    # valid_df.to_csv(valid_file, index=False)\n",
    "    # test_df.to_csv(test_file, index=False)\n",
    "    # user_df.to_csv(grid_file, index=False)\n",
    "\n",
    "    user_df.to_csv(segment_file, index=False)\n",
    "    # break\n",
    "# time_df = pd.DataFrame({\"user_id\":user_id_list,\n",
    "#                         \"begin_date\":begin_list,\n",
    "#                         \"end_date\":end_list,\n",
    "#                         \"time_grid\":time_grid_list,\n",
    "#                         \"ratio\":ratio})\n",
    "# time_df.to_csv('time_grid_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      group  mask\n1         2    55\n16       17    64\n32       33    72\n33       34   110\n42       43    66\n...     ...   ...\n1290   1291    42\n1305   1306    45\n1307   1308    46\n1328   1329    99\n1332   1333    74\n\n[155 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>group</th>\n      <th>mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1290</th>\n      <td>1291</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>1305</th>\n      <td>1306</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>1307</th>\n      <td>1308</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>1328</th>\n      <td>1329</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1332</th>\n      <td>1333</td>\n      <td>74</td>\n    </tr>\n  </tbody>\n</table>\n<p>155 rows  2 columns</p>\n</div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = pd.read_csv(segment_file)\n",
    "\n",
    "group_df = user_df.groupby('group')['mask'].sum().reset_index()\n",
    "group_num = group_df.loc[group_df['mask'] > 2*20, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:49:50.027362Z",
     "start_time": "2024-03-28T03:49:49.998846Z"
    }
   },
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       group  mask\n17      9742    64\n33     13819    72\n34     13831   110\n43     33534    66\n54     33649    77\n...      ...   ...\n1261  698420    68\n1265  698546    94\n1285  698684    64\n1329  705885    99\n1333  705983    74\n\n[93 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>group</th>\n      <th>mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>9742</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>13819</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>13831</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>33534</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>33649</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1261</th>\n      <td>698420</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>1265</th>\n      <td>698546</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>1285</th>\n      <td>698684</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>1329</th>\n      <td>705885</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>1333</th>\n      <td>705983</td>\n      <td>74</td>\n    </tr>\n  </tbody>\n</table>\n<p>93 rows  2 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df = user_df.groupby(['group'])['mask'].sum().reset_index()\n",
    "group_df.loc[group_df['mask'] >= 2*20, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T02:38:53.762056Z",
     "start_time": "2024-03-28T02:38:53.732229Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame({\"user_id\":user_id_list,\n",
    "                        # \"time_grid\":time_grid_list,\n",
    "                        \"ratio_1min\":ratio})\n",
    "time_df['user_id'] = time_df['user_id'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('time_grid_sample.csv')\n",
    "# df = pd.merge(df, time_df, on='user_id')\n",
    "# df.to_csv('time_grid_sample.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(grid_file)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "df = pd.read_csv(grid_file)\n",
    "user_df = df.copy()\n",
    "day = 8\n",
    "length = day * 28\n",
    "total_samps = 6\n",
    "\n",
    "grid_days = np.arange(int(user_df.shape[0] / day))\n",
    "print(grid_days.shape[0]//total_samps)\n",
    "grid_days = grid_days[0:(grid_days.shape[0]//total_samps) * total_samps]\n",
    "print(grid_days.shape)\n",
    "\n",
    "grid_days = grid_days.reshape(-1, total_samps)\n",
    "sample_list = np.arange(grid_days.shape[0])\n",
    "random.shuffle(sample_list)\n",
    "sample_list.shape\n",
    "# random.shuffle(grid_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "# Example coordinates\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "grid_size_meter = 500  # Size of each grid in meters\n",
    "\n",
    "mapCreator = GPSGridMapCreator(grid_size_meter)\n",
    "mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "print(mapCreator.find_grid_number(39.99, 117.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "id = 68\n",
    "\n",
    "gap = 10\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "round_time = round_sec\n",
    "\n",
    "user_id = locationPreprocessor.getUserId(id)\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "seg_file = './Data/' + user_id + '/csv/' + user_id + '_segment.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df['datetime'] = pd.to_datetime(df['date'] + \" \" + df['time'])\n",
    "df['datetime'] = df['datetime'].dt.round(round_time)\n",
    "\n",
    "df = df.set_index('datetime').reset_index()\n",
    "user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
    "\n",
    "#   \n",
    "user_df['time_diff'] = user_df['datetime'].diff()\n",
    "\n",
    "# 1    \n",
    "threshold = pd.Timedelta(minutes=1)\n",
    "user_df['segment'] = (user_df['time_diff'] > threshold).cumsum()\n",
    "user_df = user_df.drop(columns=['time_diff'])\n",
    "user_df.to_csv(seg_file, index=False)\n",
    "# user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(seg_file)\n",
    "user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/068/csv/068_origin_grid_10s.csv')\n",
    "df.head()\n",
    "df_1 = df[df.columns[0:13].to_list()].copy()\n",
    "# df_1 = pd.concat([df_1, df.iloc[:, 6:13]], axis=1) # ~ Hour + 100m\n",
    "# # df_1 = pd.get_dummies(df_1, columns=['hour'], drop_first=True)\n",
    "\n",
    "df_1 = pd.concat([df_1, df.iloc[:, 13:15]], axis=1) # 100m\n",
    "df_1 = pd.concat([df_1, df.iloc[:, 21:]], axis=1) # 2000m\n",
    "df_1 = pd.concat([df_1, df.iloc[:, 17:19]], axis=1) # 1000m\n",
    "df_1 = pd.concat([df_1, df.iloc[:, 15:17]], axis=1) # 500m\n",
    "df_1.head()\n",
    "# df_1.columns.shape\n",
    "# df.columns[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, data_dir, user_list, device, time_delta, y_timestep, length, label_attribute, sample_s, replace=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.user_list = user_list\n",
    "        self.device = device\n",
    "        self.time_delta = time_delta\n",
    "        self.y_timestep = y_timestep\n",
    "        self.length = length\n",
    "        self.label_attribute = label_attribute\n",
    "        self.sample_s = sample_s\n",
    "        self.replace = replace\n",
    "\n",
    "        self.csv_file = '_origin_grid_10s.csv'\n",
    "    \n",
    "    def sampleSet(self, dataset):\n",
    "        user_df = dataset.copy()\n",
    "        user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "    \n",
    "        #       ,   segment list \n",
    "        segment_info = user_df.groupby('segment')['datetime'].agg(['min', 'max'])\n",
    "        segment_info = segment_info.reset_index()\n",
    "        segment_info['time_gap'] = segment_info['max'] - segment_info['min']\n",
    "        segment_info['over_time_delta'] = segment_info['time_gap'] >= pd.Timedelta(minutes=self.time_delta)\n",
    "        segment_info = segment_info.loc[segment_info['over_time_delta'] == True, :]\n",
    "        segment_list = segment_info['segment'].to_list()\n",
    "\n",
    "        user_df = user_df.drop(columns=['datetime'])\n",
    "\n",
    "        # segment_list , sample  mini-batch \n",
    "        mini_batch = []\n",
    "\n",
    "        segment_list = np.random.choice(segment_list, size=self.sample_s, replace=self.replace)\n",
    "\n",
    "        for seg_num in segment_list:\n",
    "            seg_df = user_df.loc[user_df['segment'] == seg_num, :]\n",
    "            if seg_df.shape[0] < length:\n",
    "                # segment  , zero padding \n",
    "                fill_quota  = np.abs(length - seg_df.shape[0])\n",
    "                zeros_r     = np.zeros([fill_quota, seg_df.shape[1]])\n",
    "                cur_sample  = seg_df.copy()\n",
    "                cur_sample  = np.concatenate([zeros_r, seg_df], axis = 0)\n",
    "                mini_batch.append(cur_sample)\n",
    "            else:\n",
    "                # segment    \n",
    "                cur_sample = seg_df.iloc[:length, :]\n",
    "                mini_batch.append(cur_sample)\n",
    "\n",
    "        return np.array(mini_batch)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        csv_file = str(self.data_dir) + str(self.user_list[index]) + '/csv/' + str(self.user_list[index]) + self.csv_file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df_1 = df[df.columns[0:13].to_list()].copy()\n",
    "        # df_1 = pd.concat([df_1, df.iloc[:, 6:13]], axis=1) # ~ Hour + 100m\n",
    "        # df_1 = pd.get_dummies(df_1, columns=['hour'], drop_first=True)\n",
    "\n",
    "        df_1 = pd.concat([df_1, df.iloc[:, 13:15]], axis=1) # 100m\n",
    "        df_1 = pd.concat([df_1, df.iloc[:, 21:]], axis=1) # 2000m\n",
    "        df_1 = pd.concat([df_1, df.iloc[:, 17:19]], axis=1) # 1000m\n",
    "        df_1 = pd.concat([df_1, df.iloc[:, 15:17]], axis=1) # 500m\n",
    "        \n",
    "        samples = self.sampleSet(df_1)\n",
    "        \n",
    "        # task_X, task_y \n",
    "        task_X = np.array(samples[:, :-self.y_timestep, 4:])\n",
    "        task_y = np.array(samples[:, -self.y_timestep:, -self.label_attribute:])\n",
    "\n",
    "        task_X = torch.from_numpy(task_X).double()\n",
    "        task_y = torch.from_numpy(task_y).double()\n",
    "\n",
    "        return task_X, task_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        # batch     \n",
    "        #   batch   .\n",
    "        #   user    \n",
    "        return len(self.user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# segment , user mini-batch  \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "min_length = 6\n",
    "time_delta = 60\n",
    "length = min_length * time_delta\n",
    "\n",
    "y_timestep = min_length * 2\n",
    "label_attribute = 2\n",
    "\n",
    "samples_s = 10\n",
    "replace = False\n",
    "attribute = ['days','latitude', 'longitude']\n",
    "\n",
    "user_df = pd.read_csv(seg_file)\n",
    "user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "\n",
    "#       ,   segment list \n",
    "segment_info = user_df.groupby('segment')['datetime'].agg(['min', 'max'])\n",
    "segment_info = segment_info.reset_index()\n",
    "segment_info['time_gap'] = segment_info['max'] - segment_info['min']\n",
    "segment_info['over_time_delta'] = segment_info['time_gap'] >= pd.Timedelta(minutes=time_delta)\n",
    "segment_info = segment_info.loc[segment_info['over_time_delta'] == True, :]\n",
    "segment_list = segment_info['segment'].to_list()\n",
    "print(len(segment_list))\n",
    "\n",
    "# segment_list , sample  mini-batch \n",
    "mini_batch = []\n",
    "\n",
    "segment_list = np.random.choice(segment_list, size=samples_s, replace=replace)\n",
    "\n",
    "for seg_num in segment_list:\n",
    "    seg_df = user_df.loc[user_df['segment'] == seg_num, attribute]\n",
    "    if seg_df.shape[0] < length:\n",
    "        # segment  , zero padding \n",
    "        fill_quota  = np.abs(length - seg_df.shape[0])\n",
    "        zeros_r     = np.zeros([fill_quota, seg_df.shape[1]])\n",
    "        cur_sample  = seg_df.copy()\n",
    "        cur_sample  = np.concatenate([zeros_r, seg_df], axis = 0)\n",
    "        mini_batch.append(cur_sample)\n",
    "    else:\n",
    "        # segment    \n",
    "        cur_sample = seg_df.iloc[:length, :]\n",
    "        mini_batch.append(cur_sample)\n",
    "\n",
    "samples = np.array(mini_batch)\n",
    "# samples = torch.from_numpy(samples).double()\n",
    "print(f\"samples.shape: {samples.shape}\")\n",
    "# samples[:2, :5, :]\n",
    "# 8 data input  , 2 data output   ?\n",
    "# or 5 data input  , 5 data output ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# task_X, task_y \n",
    "task_X = np.array(samples[:, :-y_timestep, :])\n",
    "task_y = np.array(samples[:, -y_timestep:, -label_attribute:])\n",
    "\n",
    "task_X = torch.from_numpy(task_X).double()\n",
    "task_y = torch.from_numpy(task_y).double()\n",
    "\n",
    "# batch    , data_loader    \n",
    "task_X = torch.unsqueeze(task_X, axis = 0)\n",
    "task_y = torch.unsqueeze(task_y, axis = 0)\n",
    "print(f\"task_x: {task_X.shape}, task_y: {task_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# MLP \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_shape, y_timestep, label_attribute):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape[-2] * input_shape[-1]\n",
    "        self.y_timestep = y_timestep\n",
    "        self.label_attribute = label_attribute\n",
    "        self.output_shape = y_timestep * label_attribute\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_shape, 256, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(256, 256, dtype=torch.double)\n",
    "        self.fc3 = nn.Linear(256, self.output_shape, dtype=torch.double)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch = torch._shape_as_tensor(x)[0]\n",
    "        mini_batch = torch._shape_as_tensor(x)[1]\n",
    "        time_step = torch._shape_as_tensor(x)[2]\n",
    "        attribute = torch._shape_as_tensor(x)[3]\n",
    "\n",
    "        in_shape_new = [-1] + [time_step * attribute]\n",
    "        x = torch.reshape(x, in_shape_new)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        out_shape_new = [batch] + [-1] + [self.y_timestep] + [self.label_attribute]\n",
    "        out = torch.reshape(x, out_shape_new)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# input_data = torch.tensor(input_data.values, dtype=torch.float32)\n",
    "# target_data = torch.tensor(output_data.values, dtype=torch.float32)\n",
    "\n",
    "def metricDist(y_true, y_pred):\n",
    "    row = (y_pred[:,:,:,0] - y_true[:,:,:,0])**2\n",
    "    col = (y_pred[:,:,:,1] - y_true[:,:,:,1])**2\n",
    "    return torch.mean(row + col) ** 0.5\n",
    "\n",
    "# args\n",
    "data_dir = \"Data/\"#\"data/geolife/Data/\"\n",
    "user_list = [\"068\", \"003\", \"004\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "min_length = 6\n",
    "time_delta = 10 # the length of segment is 10mins\n",
    "length = min_length * time_delta\n",
    "\n",
    "y_timestep = min_length * 2\n",
    "x_attribute = 20\n",
    "label_attribute = 2\n",
    "sample_s = 10\n",
    "replace = False\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# Dataset\n",
    "training_data    = SegmentDataset(data_dir, user_list, device, time_delta, y_timestep, length, label_attribute, sample_s, replace)\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=False)\n",
    "\n",
    "# Model  \n",
    "model = MLP(input_shape=[(length-y_timestep), x_attribute], y_timestep = y_timestep, label_attribute=label_attribute)\n",
    "\n",
    "model = model.to(torch.double)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for train_idx, train_data in enumerate(train_dataloader, 0):\n",
    "        task_X, task_y = train_data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(task_X)\n",
    "        loss = criterion(output, task_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted = model(task_X)\n",
    "    loss = metricDist(task_y, predicted)\n",
    "    print(\"target latitude, longitude:\", task_y)\n",
    "    print(\"Predicted latitude, longitude:\", predicted)\n",
    "    print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment = user_df['segment'].unique()\n",
    "user_df.groupby('segment')['what'].value_counts() > 20\n",
    "\n",
    "# user_df.loc[user_df['segment'] == segment[3], :]\n",
    "\n",
    "# 39.968885\t116.419843\n",
    "# 39.969592\t116.418630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = user_df.copy()\n",
    "df = df.fillna(np.nan)\n",
    "if str(df.iloc[11, -2]) == 'nan':\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "df = pd.read_csv('user_data_volumn.csv')\n",
    "segment_csv = \"_segment_list_10min.csv\"\n",
    "\n",
    "seg_list = []\n",
    "for id in df['user_id']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = getUserId(id)\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "    seg_df = pd.read_csv(segment_file)\n",
    "    seg_list += [seg_df.shape[0]]\n",
    "    \n",
    "df['segment_list_10min'] = seg_list\n",
    "df.to_csv('user_data_volumn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('user_data_volumn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv  5   (rounded)\n",
    "# round_min   round_min  data \n",
    "\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "\n",
    "days_min = 0.000696\n",
    "gap = 10\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "\n",
    "# origin_grid_10min = just add grid in original csv file\n",
    "# grid_10min        = from begin to end full data with fillna(ffill)\n",
    "round_csv = '_origin_round_' + round_sec + '.csv'\n",
    "grid_csv = '_origin_grid_' + round_sec + '.csv'\n",
    "grid_list = [50, 100, 500, 1000, 1500, 2000, 3000]\n",
    "\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    rounded_file = './Data/' + user_id + '/csv/' + user_id + round_csv\n",
    "    grid_file = './Data/' + user_id + '/csv/' + user_id + grid_csv\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(df.shape)\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + \" \" + df['time'])\n",
    "    df['datetime'] = df['datetime'].dt.round(round_sec)\n",
    "\n",
    "    df = df.set_index('datetime').reset_index()\n",
    "    user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
    "    user_df = locationPreprocessor.convert_coord_for_blender_for_user(user_df)\n",
    "\n",
    "    user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "    \n",
    "    # begin = user_df['datetime'][0]\n",
    "    # end = user_df['datetime'][user_df.shape[0]-1]\n",
    "\n",
    "    # print(f'begin: {begin}')\n",
    "    # print(f'end: {end}')\n",
    "\n",
    "    # date_df = pd.DataFrame({'datetime':pd.date_range(begin, end, freq=round_min)})\n",
    "    # user_df = pd.merge(date_df, user_df, how='outer', on='datetime')\n",
    "    \n",
    "    # # add days \n",
    "    # start_days = user_df['days'][0]\n",
    "    # end_days = start_days + (date_df.shape[0] * (days_min * gap))\n",
    "    # days_list = np.arange(start_days, end_days, (days_min * gap))\n",
    "\n",
    "    # user_df['days'] = days_list[:user_df.shape[0]]\n",
    "    user_df['year'] = user_df['datetime'].dt.year\n",
    "    user_df['month'] = user_df['datetime'].dt.month\n",
    "    user_df['week'] = user_df['datetime'].dt.weekday\n",
    "    user_df['weekend'] = np.where(user_df['week'] < 5, 0, 1)\n",
    "    user_df['hour'] = user_df['datetime'].dt.hour\n",
    "    user_df['day'] = user_df['datetime'].dt.day\n",
    "    # user_df = user_df.fillna(method='ffill')\n",
    "\n",
    "    #   \n",
    "    user_df['time_diff'] = user_df['datetime'].diff()\n",
    "\n",
    "    # 1    \n",
    "    threshold = pd.Timedelta(minutes=1)\n",
    "    user_df['segment'] = (user_df['time_diff'] > threshold).cumsum()\n",
    "    # user_df = user_df.drop(columns=['time_diff'])\n",
    "\n",
    "    # save rounded file\n",
    "    # user_df.to_csv(rounded_file, index=False)\n",
    "\n",
    "    # grid process\n",
    "    user_df = user_df.drop(columns=['altitude', 'what'])\n",
    "    for grid_len in grid_list:\n",
    "        mapCreator = GPSGridMapCreator(grid_len)\n",
    "        mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "        grid_row = 'grid_row_' + str(grid_len) + 'm' # row\n",
    "        grid_col = 'grid_col_' + str(grid_len) + 'm' # column\n",
    "        grid_lat = 'grid_lat_' + str(grid_len) + 'm' # lat\n",
    "        grid_lon = 'grid_lon_' + str(grid_len) + 'm' # lon\n",
    "        grid_num = 'grid_num_' + str(grid_len) + 'm' # num      \n",
    "        user_df[grid_row], user_df[grid_col],_,_,_ = mapCreator.find_grid_number(user_df['latitude'], user_df['longitude'])\n",
    "    # save grid filen\n",
    "    user_df.to_csv(grid_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "\n",
    "# grid num - lat, lon\n",
    "mapCreator = GPSGridMapCreator(1000)\n",
    "mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "# mapCreator.get_num_lat_lon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "time_delta = 20\n",
    "segment_delta = str(time_delta) + 'min'\n",
    "segment_csv = '_segment_list_' + segment_delta + '.csv'\n",
    "segment_col = 'segment_list_' + segment_delta\n",
    "\n",
    "df = pd.read_csv('user_data_volumn.csv')\n",
    "seg_list = []\n",
    "for id in df['user_id']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = getUserId(id)\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "    seg_df = pd.read_csv(segment_file)\n",
    "    seg_list += [seg_df.shape[0]]\n",
    "    \n",
    "df[segment_col] = seg_list\n",
    "df.to_csv('user_data_volumn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_id = '003'\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '_origin_grid_10s.csv'\n",
    "segment_file = './Data/' + user_id + '/csv/' + user_id + '_segment_list_10min.csv'\n",
    "\n",
    "df = pd.read_csv(segment_file)\n",
    "\n",
    "df.sort_values('segment_list', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_list = ['068', '030', '085', '003', '004', '065', '067', '014', '050', '141', '035', '013', '039', '024', '036', '038', '096', '037', '074', '000', '179', '034', '026', '005', '001', '119', '125', '043', '011', '112', '016', '015', '007', '009', '135', '029', '008', '044', '078', '122']\n",
    "len(user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('user_data_volumn.csv')\n",
    "df.loc[df['segment_list_60min'] >= 10, :].shape\n",
    "# df.sort_values('segment_list_20min', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv  5   (rounded)\n",
    "# round_min   round_min  data \n",
    "\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "\n",
    "days_min = 0.000696\n",
    "gap = 10\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "\n",
    "time_delta = 10\n",
    "segment_delta = str(time_delta) + 'min'\n",
    "\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "\n",
    "# origin_grid_10min = just add grid in original csv file\n",
    "# grid_10min        = from begin to end full data with fillna(ffill)\n",
    "\n",
    "grid_csv = '_origin_grid_' + round_sec + '.csv'\n",
    "segment_csv = '_segment_list_' + segment_delta + '.csv'\n",
    "grid_list = [50, 100, 500, 1000, 1500, 2000, 3000]\n",
    "\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "    grid_file = './Data/' + user_id + '/csv/' + user_id + grid_csv\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + \" \" + df['time'])\n",
    "    df['datetime'] = df['datetime'].dt.round(round_sec)\n",
    "\n",
    "    df = df.set_index('datetime').reset_index()\n",
    "    user_df = df.groupby('datetime')[df.columns[1:].to_list()].mean().reset_index()\n",
    "    user_df = locationPreprocessor.convert_coord_for_blender_for_user(user_df)\n",
    "\n",
    "    user_df['datetime'] = pd.to_datetime(user_df['datetime'])\n",
    "    \n",
    "    user_df['year'] = user_df['datetime'].dt.year\n",
    "    user_df['month'] = user_df['datetime'].dt.month\n",
    "    user_df['week'] = user_df['datetime'].dt.weekday\n",
    "    user_df['weekend'] = np.where(user_df['week'] < 5, 0, 1)\n",
    "    user_df['hour'] = user_df['datetime'].dt.hour\n",
    "    user_df['day'] = user_df['datetime'].dt.day\n",
    "\n",
    "    #   \n",
    "    user_df['time_diff'] = user_df['datetime'].diff()\n",
    "\n",
    "    # 1    \n",
    "    threshold = pd.Timedelta(minutes=1)\n",
    "    user_df['segment'] = (user_df['time_diff'] > threshold).cumsum()\n",
    "\n",
    "    # save segment_list\n",
    "    #       ,   segment list \n",
    "    segment_info = user_df.groupby('segment')['datetime'].agg(['min', 'max'])\n",
    "    segment_info = segment_info.reset_index()\n",
    "    segment_info['time_gap'] = segment_info['max'] - segment_info['min']\n",
    "    segment_info['over_time_delta'] = segment_info['time_gap'] >= pd.Timedelta(minutes=time_delta)\n",
    "    segment_info = segment_info.loc[segment_info['over_time_delta'] == True, :]\n",
    "    segment_list = segment_info['segment'].to_list()\n",
    "\n",
    "    segment_df = pd.DataFrame({'segment_list':segment_list})\n",
    "    segment_df.to_csv(segment_file, index=False)\n",
    "    \n",
    "    # grid process\n",
    "    user_df = user_df.drop(columns=['altitude', 'what'])\n",
    "    for grid_len in grid_list:\n",
    "        mapCreator = GPSGridMapCreator(grid_len)\n",
    "        mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "        grid_row = 'grid_row_' + str(grid_len) + 'm' # row\n",
    "        grid_col = 'grid_col_' + str(grid_len) + 'm' # column\n",
    "        grid_lat = 'grid_lat_' + str(grid_len) + 'm' # lat\n",
    "        grid_lon = 'grid_lon_' + str(grid_len) + 'm' # lon\n",
    "        grid_num = 'grid_num_' + str(grid_len) + 'm' # num      \n",
    "        user_df[grid_row], user_df[grid_col],_,_,_ = mapCreator.find_grid_number(user_df['latitude'], user_df['longitude'])\n",
    "\n",
    "    # save grid file\n",
    "    user_df = user_df.drop(columns=['datetime', 'latitude', 'longitude'])\n",
    "    user_df.to_csv(grid_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "df = pd.read_csv('user_data_volumn.csv')\n",
    "segment_csv = \"_segment_list_10min.csv\"\n",
    "\n",
    "seg_list = []\n",
    "for id in df['user_id']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = getUserId(id)\n",
    "    segment_file = './Data/' + user_id + '/csv/' + user_id + segment_csv\n",
    "    seg_df = pd.read_csv(segment_file)\n",
    "    seg_list += [seg_df.shape[0]]\n",
    "    \n",
    "df['segment_list_10min'] = seg_list\n",
    "df.to_csv('user_data_volumn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "from gps_grid_map_creator import GPSGridMapCreator\n",
    "\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "\n",
    "grid_list = [50, 100, 500, 1000, 1500, 2000, 3000]\n",
    "\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "for grid_len in grid_list:\n",
    "    mapCreator = GPSGridMapCreator(grid_len)\n",
    "    mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "    lat, lon = mapCreator.get_num_lat_lon()\n",
    "    lat_list += [lat]\n",
    "    lon_list += [lon]\n",
    "\n",
    "df = pd.DataFrame({'grid':grid_list,\n",
    "                   'latitude':lat_list,\n",
    "                   'longitude':lon_list})\n",
    "df.to_csv('grid_map_coordinate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('grid_map_coordinate.csv')\n",
    "\n",
    "width = df.loc[df['grid']==1000, 'latitude'].values[0]\n",
    "width\n",
    "height = df.loc[df['grid']==1000, 'longitude'].values[0]\n",
    "height\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#   \n",
    "grid = 1000\n",
    "width = df.loc[df['grid']==grid, 'latitude'].values[0]\n",
    "height = df.loc[df['grid']==grid, 'longitude'].values[0]\n",
    "\n",
    "#   \n",
    "coordinates = [(54, 10), (79, 20)]\n",
    "\n",
    "#   \n",
    "plt.figure(figsize=(width/50, height/100))  #   \n",
    "plt.scatter(*zip(*coordinates), color='red', marker='o', s=10)  #  \n",
    "plt.xlim(0, width)    # x  \n",
    "plt.ylim(0, height)   # y  \n",
    "plt.gca().invert_yaxis()  # y    \n",
    "plt.title(str(grid) + '-grid')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"map_with_coordinates.png\")  #   \n",
    "plt.show()\n",
    "\n",
    "# print(\"    map_with_coordinates.png .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#   \n",
    "width = 100\n",
    "height = 100\n",
    "time = 100\n",
    "\n",
    "#   \n",
    "coordinates_x = [(30, 40, 50), (70, 80, 60), (70, 80, 70), (20, 40, 80)]\n",
    "coordinates_y = [(10, 20, 50), (30, 50, 60), (70, 80, 70), (30, 40, 80)]\n",
    "\n",
    "# 3D  \n",
    "fig = plt.figure(figsize=(width/20, height/20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#  \n",
    "for coord in coordinates_x:\n",
    "    ax.scatter(*coord, color='red', marker='o', s=10)\n",
    "\n",
    "#  \n",
    "for coord in coordinates_y:\n",
    "    ax.scatter(*coord, color='blue', marker='o', s=10)\n",
    "\n",
    "# line \n",
    "ax.plot(*zip(*coordinates_x), color='red', linestyle='--', linewidth=1)\n",
    "ax.plot(*zip(*coordinates_y), color='blue', linestyle='--', linewidth=1)\n",
    "\n",
    "#   \n",
    "ax.set_xlim(0, width)\n",
    "ax.set_ylim(0, height)\n",
    "ax.set_zlim(0, time)\n",
    "\n",
    "#   \n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Time\")\n",
    "\n",
    "#    \n",
    "for angle in range(0, 360, 10):\n",
    "    ax.view_init(elev=20, azim=angle)  #   \n",
    "\n",
    "# plt.title(\"3d-map\")\n",
    "# # plt.savefig(\"3d_map_with_coordinates.png\")  #   \n",
    "# plt.show()\n",
    "\n",
    "# print(\"   3D  3d_map_with_coordinates.png .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "days_min = 0.000696\n",
    "gap = 10\n",
    "round_min = str(gap) + 'min'\n",
    "round_sec = str(gap) + 's'\n",
    "grid_csv = '_origin_grid_' + round_sec + '.csv'\n",
    "user_id = '035'\n",
    "grid_file = './Data/' + user_id + '/csv/' + user_id + grid_csv\n",
    "\n",
    "df = pd.read_csv(grid_file)\n",
    "df = df.drop(columns=['time_diff'])\n",
    "df.iloc[:, 19:21]\n",
    "# columns = ['datetime','latitude', 'longitude', 'days']\n",
    "# df_1 = df[df.columns[0:14].to_list()].copy()\n",
    "# df_1 = df[columns].copy()\n",
    "# df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#   \n",
    "width = 100\n",
    "height = 100\n",
    "\n",
    "#    ( )\n",
    "coordinates = [\n",
    "    ((10, 10), time.time()),  #    ( )\n",
    "    ((15, 15), time.time() + 5)  #    (5  )\n",
    "]\n",
    "\n",
    "#   \n",
    "plt.figure(figsize=(width/20, height/20))  #   \n",
    "plt.xlim(0, width)  # x  \n",
    "plt.ylim(0, height)  # y  \n",
    "plt.gca().invert_yaxis()  # y    \n",
    "plt.title(\"a\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(True)\n",
    "\n",
    "for coord, timestamp in coordinates:\n",
    "    plt.scatter(*coord, color='red', marker='o', s=100)  #  \n",
    "    # plt.savefig(f\"map_with_coordinates_{timestamp:.0f}.png\")  #   \n",
    "    plt.pause(1)  # 1 \n",
    "    plt.clf()  #  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.csv  column  \n",
    "import pandas as pd\n",
    "\n",
    "user_id = '067'\n",
    "\n",
    "gap = 10\n",
    "round_min = str(gap) + 'min'\n",
    "grid_csv = '_grid_' + round_min + '.csv'\n",
    "grid_file = './Data/' + user_id + '/csv/' + user_id + grid_csv\n",
    "\n",
    "train_file = './Data/' + user_id + '/csv/' + user_id + '_train_set.csv'\n",
    "valid_file = './Data/' + user_id + '/csv/' + user_id + '_valid_set.csv'\n",
    "test_file = './Data/' + user_id + '/csv/' + user_id + '_test_set.csv'\n",
    "\n",
    "df = pd.read_csv(grid_file)\n",
    "print(df.info())\n",
    "# print(df.tail())\n",
    "\n",
    "# columns = df.loc[:, :'grid_col_50m'].columns.to_list() #df.columns[:'grid_col_50m'].to_list() #+ df.columns[58:60].to_list()\n",
    "columns = df.columns[2:11].to_list() #+ df.columns[15:17].to_list()\n",
    "df_1 = df[columns].copy()\n",
    "df_1 = pd.get_dummies(df_1, columns=['hour'], drop_first=True)\n",
    "df_1 = pd.concat([df_1, df.iloc[:, 15:17]], axis=1)\n",
    "print(df_1.columns)\n",
    "print(df_1.tail())\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_valid = pd.read_csv(valid_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "print(f\"train: {df_train.shape[0]}\")\n",
    "print(f\"valid: {df_valid.shape[0]}\")\n",
    "print(f\"test: {df_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "user_id_list = []\n",
    "df_len_list = []\n",
    "ori_len_list = []\n",
    "begin_day_list = []\n",
    "end_day_list = []\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    print(f\"user_id: {id}\")\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    orig_df = pd.read_csv(csv_file)\n",
    "    orig_df['datetime'] = pd.to_datetime(orig_df['date'] + \" \" + orig_df['time'])\n",
    "\n",
    "    user_id_list += [user_id]\n",
    "    df_len_list += [orig_df.shape[0]]\n",
    "    ori_len_list += [orig_df.shape[0]]\n",
    "\n",
    "    begin_day_list += [orig_df.iloc[0, -1]]\n",
    "    end_day_list += [orig_df.iloc[-1, -1]]\n",
    "\n",
    "user_df = pd.DataFrame({'user_id':user_id_list,\n",
    "                        'begin_date':begin_day_list,\n",
    "                        'end_date':end_day_list,\n",
    "                        'original_len':ori_len_list,\n",
    "                        'rounded_len':df_len_list})\n",
    "user_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 100     user 45\n",
    "# user_df.to_csv('origin_round_10min_user_list.csv', index=False)\n",
    "\n",
    "df_1 = pd.read_csv('origin_round_10min_user_list.csv')\n",
    "df = pd.read_csv('round_10min_user_list.csv')\n",
    "\n",
    "df = df.merge(df_1, how='inner', on = 'user_id')\n",
    "df = df.loc[df['rounded_len_y'] > 2000, :]\n",
    "df['extra_ratio'] = round(df['rounded_len_x'] / df['rounded_len_y'], 1)\n",
    "\n",
    "df = df.sort_values(['extra_ratio'], ascending=True)\n",
    "\n",
    "df = df.rename(columns = {'rounded_len_y':'rounded_10min_len',\n",
    "                          'rounded_len_x':'date_rounded',\n",
    "                          'original_len_x':'original_len'})\n",
    "df = df[['user_id','begin_date','end_date','date_rounded','original_len', 'rounded_10min_len', 'extra_ratio']]\n",
    "df.head()\n",
    "df.to_csv('extra_ratio.csv', index=False)\n",
    "\n",
    "# df = df.loc[df['rounded_len'] >= 14400, :]\n",
    "# df['user_id'].to_list()\n",
    "# df.head(20)\n",
    "# df.sort_values('rounded_len', ascending=False)\n",
    "# ['068', '030', '085', '003', '004']\",\"['065', '067']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user = pd.read_csv('extra_ratio.csv')\n",
    "user.head(10)\n",
    "# user = user.sort_values('original_len', ascending=False)\n",
    "# user = user[['user_id', 'original_len', 'extra_rounded', 'rounded_10min', 'extra_ratio']]\n",
    "# user.rename(columns={'extra_rounded':'rounded_10min',\n",
    "#                      'rounded_10min':'rounded_len'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_id = '085'\n",
    "csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "user_df = pd.read_csv(csv_convert_file)\n",
    "user_df.head(1)\n",
    "\n",
    "# user_df = user_df[['days','month','week','weekend','hour','day','x','y']]\n",
    "# user_df = pd.get_dummies(user_df, columns=['week','month'], drop_first=True)\n",
    "# user_xy = user_df.loc[:, ['x', 'y']]\n",
    "# user_df = user_df.drop(columns=['x', 'y'])\n",
    "# pd.concat([user_df, user_xy], axis=1)\n",
    "\n",
    "df_1 = user_df[['days','month','week','weekend','hour','day','x','y']]\n",
    "df_1 = pd.get_dummies(df_1, columns=['week','month'], drop_first=True)\n",
    "df_xy = df_1.loc[:, ['x', 'y']]\n",
    "df_1 = df_1.drop(columns=['x', 'y'])\n",
    "df_1 = pd.concat([df_1, df_xy], axis=1)\n",
    "df_1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame({\"user_id\":user_id_list,\n",
    "                        \"data_vol\":df_len_list})\n",
    "user_df = user_df.sort_values(['data_vol'], ascending=False)\n",
    "user_df.to_csv('user_data_volumn.csv', index=False)\n",
    "\n",
    "print(user_df.head(5))\n",
    "user_df.iloc[:5, 0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_s = 1\n",
    "sample_q = 1\n",
    "\n",
    "args_epoch = 1200\n",
    "args_patience = 300\n",
    "\n",
    "gap_min = 12 # 1 min\n",
    "gap = gap_min\n",
    "\n",
    "y_timestep = 100 # must be less than length\n",
    "length = 15000\n",
    "\n",
    "train_list      = ['068', '030', '085', '004']#, '085']\n",
    "validation_list = ['067']#, '085']\n",
    "test_list       = ['067']#, '085']\n",
    "\n",
    "conf_df = pd.DataFrame({'sample_s':[sample_s],\n",
    "                        'sample_q':[sample_q],\n",
    "                        'epoch':[args_epoch],\n",
    "                        'patience':[args_patience],\n",
    "                        'gap':[gap],\n",
    "                        'y_timestep':[y_timestep],\n",
    "                        'length':[length],\n",
    "                        'train_list':[train_list],\n",
    "                        'val_list':[validation_list],\n",
    "                        'test_list':[test_list]})\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.loc[user_df['user_id'] == '001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location_df_pre = user_location_df.copy()\n",
    "user_location_df_pre = user_location_df_pre.set_index('user_id', drop=True).copy()\n",
    "user_location_df_pre.head(2)\n",
    "\n",
    "X = ['min_lat', 'min_lon', 'max_lat', 'max_lon']\n",
    "q1 = user_location_df_pre[X].quantile(0.25)\n",
    "q3 = user_location_df_pre[X].quantile(0.75)\n",
    "iqr = (q3-q1) * 1.5\n",
    "\n",
    "cond1 = user_location_df_pre[X] >= (q1 - iqr)\n",
    "user_location_df_pre = user_location_df_pre[cond1].dropna().copy()\n",
    "print(user_location_df_pre.shape)\n",
    "\n",
    "cond2 = user_location_df_pre[X] <= (q3 + iqr)\n",
    "user_location_df_pre = user_location_df_pre[cond2].dropna().copy()\n",
    "print(user_location_df_pre.shape)\n",
    "user_location_df_pre.min()\n",
    "user_location_df_pre.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "PI = 3.14159265358979323846\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2, unit):\n",
    "    deg2rad_multiplier = PI / 180\n",
    "    lat1 = lat1 * deg2rad_multiplier\n",
    "    lon1 = lon1 * deg2rad_multiplier\n",
    "    lat2 = lat2 * deg2rad_multiplier\n",
    "    lon2 = lon2 * deg2rad_multiplier\n",
    "\n",
    "    radius = 6378.137  # Earth mean radius defined by WGS84\n",
    "    dlon = lon2 - lon1\n",
    "    distance = math.acos(math.sin(lat1) * math.sin(lat2) + math.cos(lat1) * math.cos(lat2) * math.cos(dlon)) * radius\n",
    "    \n",
    "    # (kilometers, miles, nautical miles)\n",
    "    if unit == 'K':\n",
    "        return distance\n",
    "    elif unit == 'M':\n",
    "        return distance * 0.621371192\n",
    "    elif unit == 'N':\n",
    "        return distance * 0.539956803\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Example usage:\n",
    "result = distance(37.7749, -122.4194, 34.0522, -118.2437, 'K') * 1000\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = 39.975300\n",
    "min_lon = 116.452488\n",
    "max_lat = 41.367085\n",
    "max_lon = 122.651456\n",
    "\n",
    "width = round(distance(39.975300, 122.651456, 41.367085, 122.651456, 'K'), 3)\n",
    "height = round(distance(41.367085, 116.452488, 41.367085, 122.651456, 'K'), 3)\n",
    "print(width)\n",
    "print(height)\n",
    "# up_width = 154.933\n",
    "# down_width = 154.933\n",
    "# left_height = 528.706\n",
    "# right_hegith = 517.778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class GPSGridMapCreator():\n",
    "    def __init__(self, grid_size_meter):\n",
    "        self.grid_size_meter = grid_size_meter\n",
    "        self.lat1 = 0\n",
    "        self.lon1 = 0\n",
    "        self.grid_numbers = 0\n",
    "        self.lat_degrees = 0\n",
    "        self.long_degrees = 0\n",
    "        self.num_lat = 0\n",
    "        self.num_lon = 0\n",
    "        \n",
    "    def km_to_degrees(self, latitude, kilometers):\n",
    "        # Earth's radius in kilometers\n",
    "        earth_radius_km = 6371.0\n",
    "\n",
    "        # Convert kilometers to radians\n",
    "        angle_rad = kilometers / earth_radius_km\n",
    "\n",
    "        # Convert radians to degrees\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "        # Correction factor for latitude\n",
    "        lat_correction = np.cos(np.radians(latitude))\n",
    "\n",
    "        # Convert degrees to adjusted degrees\n",
    "        adjusted_degrees = angle_deg / lat_correction\n",
    "\n",
    "        return adjusted_degrees\n",
    "\n",
    "    def meter_to_degrees(self, latitude, meters):\n",
    "        # Convert meters to kilometers\n",
    "        kilometers = meters / 1000\n",
    "\n",
    "        # Convert kilometers to degrees using the km_to_degrees function\n",
    "        degrees = self.km_to_degrees(latitude, kilometers)\n",
    "\n",
    "        return degrees\n",
    "\n",
    "    def create_grid_map(self, lat1, lon1, lat2, lon2):\n",
    "        self.lat1 = lat1\n",
    "        self.lon1 = lon1\n",
    "        # Convert grid size from meters to degrees\n",
    "        self.lat_degrees = self.meter_to_degrees((lat1 + lat2) / 2, self.grid_size_meter)\n",
    "        self.lon_degrees = self.meter_to_degrees((lon1 + lon2) / 2, self.grid_size_meter)\n",
    "\n",
    "        # print(f\"lat_degrees: {self.lat_degrees}, lon_degree: {self.lon_degrees}\")\n",
    "        # Calculate the number of grid points in latitude and longitude directions\n",
    "        self.num_lat = int(np.abs(lat2 - lat1) / np.abs(self.lat_degrees))\n",
    "        self.num_lon = int(np.abs(lon2 - lon1) / np.abs(self.lon_degrees))\n",
    "        # print(f\"lon2 - lon1: {np.abs(lon2 - lon1)}\")\n",
    "        # print(f\"num_lat: {self.num_lat}, num_lon: {self.num_lon}\")\n",
    "\n",
    "        # Generate latitude and longitude grid points\n",
    "        latitudes = np.linspace(lat1, lat2, self.num_lat)\n",
    "        longitudes = np.linspace(lon1, lon2, self.num_lon)\n",
    "\n",
    "        # print(f\"the num of latitudes: {len(latitudes)}\")\n",
    "        # print(f\"the num of longitude: {len(longitudes)}\")\n",
    "        # print(latitudes)\n",
    "        # print(longitudes)\n",
    "        # Create a 2D grid for numbering\n",
    "        self.grid_numbers = np.arange(0, (self.num_lat + 1) * (self.num_lon + 1)).reshape(self.num_lat + 1, self.num_lon + 1)\n",
    "        print(f\"gird_number: {self.grid_numbers.shape[0] * self.grid_numbers.shape[1]}\")\n",
    "\n",
    "    def find_grid_number(self, lat, lon):\n",
    "        grid_lat = int((lat - self.lat1) / np.abs(self.lat_degrees))\n",
    "        grid_lon = int((lon - self.lon1) / np.abs(self.lon_degrees))\n",
    "        gird_number = grid_lat * (self.num_lon + 1) + grid_lon + 1\n",
    "        return gird_number, grid_lat, grid_lon\n",
    "\n",
    "# Example coordinates\n",
    "lat1, lon1 = 39.975300, 116.452488  # Lower-left corner\n",
    "lat2, lon2 = 41.367085, 122.651456  # Upper-right corner\n",
    "grid_size_meter = 100  # Size of each grid in meters\n",
    "\n",
    "mapCreator = GPSGridMapCreator(grid_size_meter)\n",
    "mapCreator.create_grid_map(lat1, lon1, lat2, lon2)\n",
    "print(mapCreator.find_grid_number(39.99, 117.5))\n",
    "print(\"done\")\n",
    "# 3991600\n",
    "# 15966400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "def km_to_degrees(latitude, kilometers):\n",
    "    # Earth's radius in kilometers\n",
    "    earth_radius_km = 6371.0\n",
    "\n",
    "    # Convert kilometers to radians\n",
    "    angle_rad = kilometers / earth_radius_km\n",
    "\n",
    "    # Convert radians to degrees\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    # Correction factor for latitude\n",
    "    lat_correction = np.cos(np.radians(latitude))\n",
    "\n",
    "    # Convert degrees to adjusted degrees\n",
    "    adjusted_degrees = angle_deg / lat_correction\n",
    "\n",
    "    return adjusted_degrees\n",
    "\n",
    "def meter_to_degrees(latitude, meters):\n",
    "    # Convert meters to kilometers\n",
    "    kilometers = meters / 1000\n",
    "\n",
    "    # Convert kilometers to degrees using the km_to_degrees function\n",
    "    degrees = km_to_degrees(latitude, kilometers)\n",
    "\n",
    "    return degrees\n",
    "\n",
    "def create_grid_map(lat1, lon1, lat2, lon2, grid_size_meter):\n",
    "    # Convert grid size from meters to degrees\n",
    "    lat_degrees = meter_to_degrees((lat1 + lat2) / 2, grid_size_meter)\n",
    "    lon_degrees = meter_to_degrees((lon1 + lon2) / 2, grid_size_meter)\n",
    "\n",
    "    print(f\"lat_degrees: {lat_degrees}, lon_degree: {lon_degrees}\")\n",
    "    # Calculate the number of grid points in latitude and longitude directions\n",
    "    num_lat = int(np.abs(lat2 - lat1) / np.abs(lat_degrees))\n",
    "    num_lon = int(np.abs(lon2 - lon1) / np.abs(lon_degrees))\n",
    "    print(f\"lon2 - lon1: {np.abs(lon2 - lon1)}\")\n",
    "    print(f\"num_lat: {num_lat}, num_lon: {num_lon}\")\n",
    "\n",
    "    # Calculate the latitude and longitude increments\n",
    "    # lat_increment = (lat2 - lat1) / num_lat\n",
    "    # lon_increment = (lon2 - lon1) / num_lon\n",
    "\n",
    "    # Generate latitude and longitude grid points\n",
    "    latitudes = np.linspace(lat1, lat2, num_lat)\n",
    "    longitudes = np.linspace(lon1, lon2, num_lon)\n",
    "\n",
    "    print(f\"the num of latitudes: {len(latitudes)}\")\n",
    "    print(f\"the num of longitude: {len(longitudes)}\")\n",
    "    print(longitudes)\n",
    "    # Create a 2D grid for numbering\n",
    "    grid_numbers = np.arange(0, (num_lat + 1) * (num_lon + 1)).reshape(num_lat + 1, num_lon + 1)\n",
    "\n",
    "    # # Plot the grid lines\n",
    "    # for lat in latitudes:\n",
    "    #     ax.plot([lon1, lon2], [lat, lat], color='black', linewidth=0.5, transform=ccrs.PlateCarree())\n",
    "    # for lon in longitudes:\n",
    "    #     ax.plot([lon, lon], [lat1, lat2], color='black', linewidth=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot the numbers on the grid\n",
    "    # for i in range(num_lat + 1):\n",
    "    #     for j in range(num_lon + 1):\n",
    "    #         ax.text(lon1 + j * lon_increment, lat1 + i * lat_increment, str(grid_numbers[i, j]),\n",
    "    #                 horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # ax.coastlines()\n",
    "    # plt.show()\n",
    "\n",
    "# Example coordinates\n",
    "lat1, lon1 = 37.0, -122.0  # Lower-left corner\n",
    "lat2, lon2 = 38.0, -121.0  # Upper-right corner\n",
    "grid_size_meter = 1000  # Size of each grid in meters\n",
    "\n",
    "create_grid_map(lat1, lon1, lat2, lon2, grid_size_meter)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "def km_to_degrees(latitude, kilometers):\n",
    "    earth_radius_km = 6371.0\n",
    "    angle_rad = kilometers / earth_radius_km\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    lat_correction = np.cos(np.radians(latitude))\n",
    "    adjusted_degrees = angle_deg / lat_correction\n",
    "    return adjusted_degrees\n",
    "\n",
    "def meter_to_degrees(latitude, meters):\n",
    "    kilometers = meters / 1000\n",
    "    degrees = km_to_degrees(latitude, kilometers)\n",
    "    return degrees\n",
    "\n",
    "def find_grid_number(lat, lon, lat1, lon1, lat_degrees, lon_degrees):\n",
    "    grid_lat = int((lat - lat1) / lat_degrees)\n",
    "    grid_lon = int((lon - lon1) / lon_degrees)\n",
    "    return grid_lat, grid_lon\n",
    "\n",
    "def create_grid_map(lat1, lon1, lat2, lon2, grid_size_meter):\n",
    "    lat_degrees = meter_to_degrees((lat1 + lat2) / 2, grid_size_meter)\n",
    "    lon_degrees = meter_to_degrees((lon1 + lon2) / 2, grid_size_meter)\n",
    "\n",
    "    num_lat = int(np.abs(lat2 - lat1) / lat_degrees)\n",
    "    num_lon = int(np.abs(lon2 - lon1) / lon_degrees)\n",
    "\n",
    "    lat_increment = (lat2 - lat1) / num_lat\n",
    "    lon_increment = (lon2 - lon1) / num_lon\n",
    "\n",
    "    latitudes = np.linspace(lat1, lat2, num_lat)\n",
    "    longitudes = np.linspace(lon1, lon2, num_lon)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.stock_img()\n",
    "\n",
    "    for lat in latitudes:\n",
    "        ax.plot([lon1, lon2], [lat, lat], color='black', linewidth=0.5, transform=ccrs.PlateCarree())\n",
    "    for lon in longitudes:\n",
    "        ax.plot([lon, lon], [lat1, lat2], color='black', linewidth=0.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "    for i in range(num_lat + 1):\n",
    "        for j in range(num_lon + 1):\n",
    "            ax.text(lon1 + j * lon_increment, lat1 + i * lat_increment, str(i * (num_lon + 1) + j + 1),\n",
    "                    horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Example input coordinates\n",
    "    input_coordinates = [(37.5, -121.5), (37.3, -122.3), (37.7, -122.7)]\n",
    "    for lat, lon in input_coordinates:\n",
    "        grid_lat, grid_lon = find_grid_number(lat, lon, lat1, lon1, lat_degrees, lon_degrees)\n",
    "        ax.text(lon, lat, f\"{grid_lat * (num_lon + 1) + grid_lon + 1} ({grid_lat},{grid_lon})\",\n",
    "                horizontalalignment='center', verticalalignment='center', color='red', fontsize=8, transform=ccrs.PlateCarree())\n",
    "\n",
    "    ax.coastlines()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example coordinates\n",
    "lat1, lon1 = 37.0, -122.0  # Lower-left corner\n",
    "lat2, lon2 = 38.0, -121.0  # Upper-right corner\n",
    "grid_size_meter = 1000  # Size of each grid in meters\n",
    "\n",
    "create_grid_map(lat1, lon1, lat2, lon2, grid_size_meter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_user_list = user_location_df_pre.index\n",
    "valid_user_list = pd.DataFrame({'valid_user_list':user_location_df_pre.index})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = 1000000 \n",
    "min_lon = 1000000\n",
    "max_lat = 0\n",
    "max_lon = 0\n",
    "\n",
    "X = ['latitude', 'longitude', 'x', 'y']\n",
    "user_min_df = pd.DataFrame(columns=X)\n",
    "user_max_df = pd.DataFrame(columns=X)\n",
    "for user_id in valid_user_list['valid_user_list']:\n",
    "    # user_id = getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "    user0 = pd.read_csv(csv_file)\n",
    "    min_df = pd.DataFrame(data=user0[X].min()).transpose()\n",
    "    max_df = pd.DataFrame(data=user0[X].max()).transpose()\n",
    "    \n",
    "    user_min_df = pd.concat([user_min_df, min_df])\n",
    "    user_max_df = pd.concat([user_max_df, max_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_min_df[['x', 'y']].min())\n",
    "print(user_max_df[['x', 'y']].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min = pd.DataFrame(user_min_df.min()).transpose()\n",
    "df_max = pd.DataFrame(user_min_df.max()).transpose()\n",
    "df_max_min = pd.DataFrame(user_min_df.max()-user_min_df.min()).transpose()\n",
    "df_diff = pd.concat([df_min, df_max, df_max_min])\n",
    "df_diff['label'] = ['min', 'max', 'max-min']\n",
    "\n",
    "round(df_diff.set_index('label'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as seaborn\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import datetime as dt\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "user_id = locationPreprocessor.getUserId(1)\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "user = pd.read_csv(csv_convert_file)\n",
    "\n",
    "user_df = user[['latitude', 'longitude', 'x', 'y', 'days', 'time']].copy()\n",
    "\n",
    "idx_list = []\n",
    "for idx in range(user_df.shape[0]):\n",
    "    if idx % 60 == 0: # 5 mins\n",
    "        idx_list += [idx]\n",
    "len(idx_list)\n",
    "\n",
    "idx_list_partial = idx_list[-200:]\n",
    "user_df_1 = user_df.iloc[idx_list_partial, :].copy()\n",
    "\n",
    "plt.figure(figsize=(60, 12))\n",
    "axes = plt.axes(projection='3d')\n",
    "axes.view_init(elev=10, azim=-80)\n",
    "\n",
    "# axes.scatter3D(user_df_1['days'], user_df_1['x'], user_df_1['y'], s=20)\n",
    "# axes.plot3D(user_df_1['days'], user_df_1['x'], user_df_1['y'])\n",
    "axes.scatter3D(user_df_1['days'], user_df_1['latitude'], user_df_1['longitude'], s=20)\n",
    "axes.plot3D(user_df_1['days'], user_df_1['latitude'], user_df_1['longitude'])\n",
    "\n",
    "axes.set_xlabel('time')\n",
    "# axes.set_ylabel('X')\n",
    "# axes.set_zlabel('Y')\n",
    "axes.set_ylabel('latitude')\n",
    "axes.set_zlabel('longitude')\n",
    "\n",
    "user_df_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df\n",
    "task_X = user_df.copy()\n",
    "task_y = task_X.iloc[-5:, -2:].copy()\n",
    "\n",
    "task_y.iloc[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geopandas: https://datascientyst.com/plot-latitude-longitude-pandas-dataframe-python/\n",
    "# pip install geopandas\n",
    "# pip install Shapely\n",
    "\n",
    "# folium: https://aboutnlp.tistory.com/33\n",
    "\n",
    "import pandas as pd\n",
    "from convert_minmax_location import LocationPreprocessor\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "from branca.element import Figure\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "\n",
    "valid_user_list = locationPreprocessor.get_valid_user_list()\n",
    "\n",
    "fig = Figure(width=550, height=350)\n",
    "for id in valid_user_list['valid_user_list']:\n",
    "    print(f\"id: {id}\")\n",
    "    user_id = locationPreprocessor.getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "    user = pd.read_csv(csv_file)\n",
    "    user_location_list = user[['latitude', 'longitude']].values.tolist()\n",
    "    center = user_location_list[0]\n",
    "    \n",
    "    map = folium.Map(location=center,\n",
    "                     zoom_start=10)\n",
    "    fig.add_child(map)\n",
    "    folium.PolyLine(locations = user_location_list,).add_to(map)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://anweh.tistory.com/17\n",
    "\n",
    "user_id = locationPreprocessor.getUserId(1)\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "user = pd.read_csv(csv_convert_file)\n",
    "print(user.head())\n",
    "user_coords = user[['latitude', 'longitude']].values.tolist()\n",
    "lat = user['latitude'].mean()\n",
    "lon = user['longitude'].mean()\n",
    "center = [lat, lon]\n",
    "\n",
    "map = folium.Map(location=center,\n",
    "                    zoom_start=9)\n",
    "k = 0\n",
    "for i in range(len(user_coords)):\n",
    "    if (i % 60) == 0:\n",
    "        _color = '#' + str(k)\n",
    "        k += 1\n",
    "        folium.Circle(\n",
    "            location = user_coords[i],\n",
    "            radius = 20,\n",
    "            # fill_color = 'Reds'\n",
    "            # color = 'Reds', #'#000000',\n",
    "            tooltip = user.iloc[i, -2:],\n",
    "            fill = 'crimson',\n",
    "        ).add_to(map)\n",
    "map.save('map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_minmax_location import LocationPreprocessor\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as seaborn\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import datetime as dt\n",
    "\n",
    "locationPreprocessor = LocationPreprocessor()\n",
    "user_id = locationPreprocessor.getUserId(29)\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "user = pd.read_csv(csv_convert_file)\n",
    "print(user.shape[0])\n",
    "print(user.head(10))\n",
    "# user = user.drop_duplicates(subset=['date', 'time'])\n",
    "user = user.drop_duplicates(subset=['days'])\n",
    "print(user.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user[['latitude', 'longitude', 'x', 'y', 'days', 'time']].copy()\n",
    "\n",
    "idx_list = []\n",
    "for idx in range(user_df.shape[0]):\n",
    "    if idx % 120 == 0:\n",
    "        idx_list += [idx]\n",
    "len(idx_list)\n",
    "\n",
    "idx_list_partial = idx_list[-50:]\n",
    "user_df_1 = user_df.iloc[idx_list_partial, :].copy()\n",
    "user_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 12))\n",
    "axes = plt.axes(projection='3d')\n",
    "axes.view_init(elev=10, azim=-80)\n",
    "\n",
    "axes.scatter3D(user_df_1['days'], user_df_1['x'], user_df_1['y'], s=20)\n",
    "axes.plot3D(user_df_1['days'], user_df_1['x'], user_df_1['y'])\n",
    "axes.set_xlabel('time')\n",
    "axes.set_ylabel('latitude')\n",
    "axes.set_zlabel('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 12))\n",
    "axes = plt.axes(projection='3d')\n",
    "axes.view_init(elev=10, azim=-80)\n",
    "\n",
    "# axes.scatter3D(user_df_1['days'], user_df_1['x'], user_df_1['y'], s=10)\n",
    "# axes.plot3D(user_df_1['days'], user_df_1['x'], user_df_1['y'])\n",
    "axes.scatter3D(user_df_1['days'], user_df_1['latitude'], user_df_1['longitude'], s=10)\n",
    "axes.plot3D(user_df_1['days'], user_df_1['latitude'], user_df_1['longitude'])\n",
    "axes.set_xlabel('time')\n",
    "axes.set_ylabel('latitude')\n",
    "axes.set_zlabel('longitude')\n",
    "# axes.set_xticklabels(one_hour['datetime'], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as seaborn\n",
    "\n",
    "import matplotlib.dates as dates\n",
    "import datetime as dt\n",
    "\n",
    "user_id = locationPreprocessor.getUserId(1)\n",
    "csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "csv_convert_file = './Data/' + user_id + '/csv/' + user_id + '_converted.csv'\n",
    "user = pd.read_csv(csv_convert_file)\n",
    "print(user.head())\n",
    "user_coords = user[['latitude', 'longitude']].values.tolist()\n",
    "\n",
    "begin_hour = user_sub.iloc[0, -1]\n",
    "end_hour = user_sub.iloc[-1, -1]\n",
    "\n",
    "print('begin_hour:', begin_hour, ', end_hour:', end_hour)\n",
    "count = 1\n",
    "while begin_hour <= end_hour:\n",
    "    condtion = (user_sub['hour'] >= begin_hour) & (user_sub['hour'] <= begin_hour + count)\n",
    "    one_hour = user_sub.loc[condtion, :]\n",
    "    begin_hour = begin_hour + count\n",
    "    if one_hour.shape[0] < 1:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(60, 12))\n",
    "    axes = plt.axes(projection='3d')\n",
    "    axes.view_init(elev=10, azim=-80)\n",
    "\n",
    "    one_hour['time_reg'] = [str(dd.time()) for dd in one_hour['datetime']]\n",
    "    axes.scatter3D(dates.date2num(one_hour['datetime']), one_hour['latitude'], one_hour['longitude'], s=10)\n",
    "    axes.plot3D(dates.date2num(one_hour['datetime']), one_hour['latitude'], one_hour['longitude'])\n",
    "    axes.set_xlabel('time')\n",
    "    axes.set_ylabel('latitude')\n",
    "    axes.set_zlabel('longitude')\n",
    "    axes.set_xticklabels(one_hour['datetime'], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location(Latitude, Logitude)   ,   .\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "min_lat = 1000000 \n",
    "min_lon = 1000000\n",
    "max_lat = 0\n",
    "max_lon = 0\n",
    "\n",
    "min_lat_list = []\n",
    "min_lon_list = []\n",
    "max_lat_list = []\n",
    "max_lon_list = []\n",
    "\n",
    "user_id_list = []\n",
    "for id in range(182):\n",
    "    user_id = getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if df.shape[0] < 500:\n",
    "        continue\n",
    "\n",
    "    user0 = df[['days', 'latitude', 'longitude']].copy()\n",
    "    user_id_list += [user_id]\n",
    "    min_lat_list += [user0['latitude'].min()]\n",
    "    max_lat_list += [user0['latitude'].max()]\n",
    "    min_lon_list += [user0['longitude'].min()]\n",
    "    max_lon_list += [user0['longitude'].max()]\n",
    "    \n",
    "    # if user0['latitude'].min() < min_lat:\n",
    "    #     min_lat = user0['latitude'].min()\n",
    "    # if user0['latitude'].max() > max_lat:\n",
    "    #     max_lat = user0['latitude'].max()\n",
    "    # if user0['longitude'].min() < min_lon:\n",
    "    #     min_lon = user0['longitude'].min()\n",
    "    # if user0['longitude'].max() > max_lon:\n",
    "    #     max_lon = user0['longitude'].max()\n",
    "\n",
    "print(f\"min_lat: {min_lat}, min_lon: {min_lon}\")\n",
    "print(f\"max_lat: {max_lat}, max_lon: {max_lon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location_df = pd.DataFrame({'user_id':user_id_list,\n",
    "                                'min_lat':min_lat_list,\n",
    "                                'min_lon':min_lon_list,\n",
    "                                'max_lat':max_lat_list,\n",
    "                                'max_lon':max_lon_list})\n",
    "user_location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_location_df_pre = user_location_df.copy()\n",
    "user_location_df_pre = user_location_df_pre.set_index('user_id', drop=True).copy()\n",
    "\n",
    "X = ['min_lat', 'min_lon', 'max_lat', 'max_lon']\n",
    "q1 = user_location_df_pre[X].quantile(0.25)\n",
    "q3 = user_location_df_pre[X].quantile(0.75)\n",
    "iqr = (q3-q1) * 1.5\n",
    "\n",
    "cond1 = user_location_df_pre[X] >= (q1 - iqr)\n",
    "# cond2 = user_location_df_pre[X] <= (q3 + iqr)\n",
    "\n",
    "user_location_df_pre = user_location_df_pre[cond1].dropna().copy()\n",
    "cond2 = user_location_df_pre[X] <= (q3 + iqr)\n",
    "user_location_df_pre = user_location_df_pre[cond2].dropna()\n",
    "valid_user_list = user_location_df_pre.index\n",
    "print(f'valid user list: {valid_user_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = pd.DataFrame({'valid_user_list':valid_user_list})\n",
    "df.to_csv('valid_user_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_user = pd.read_csv('valid_user_list.csv')\n",
    "valid_user['valid_user_list']\n",
    "\n",
    "# location(Latitude, Logitude)   ,   .\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getUserId(id):\n",
    "    val = \"\"\n",
    "    if id < 10:\n",
    "        val += \"00\"\n",
    "        val += str(id)\n",
    "    elif id < 100:\n",
    "        val += \"0\"\n",
    "        val += str(id)\n",
    "    else:\n",
    "        val = str(id)\n",
    "    return val\n",
    "\n",
    "min_lat = 1000000 \n",
    "min_lon = 1000000\n",
    "max_lat = 0\n",
    "max_lon = 0\n",
    "\n",
    "for id in valid_user['valid_user_list']:\n",
    "    user_id = getUserId(id)\n",
    "    csv_file = './Data/' + user_id + '/csv/' + user_id + '.csv'\n",
    "    user0 = pd.read_csv(csv_file)\n",
    "    \n",
    "    if user0['latitude'].min() < min_lat:\n",
    "        min_lat = user0['latitude'].min()\n",
    "    if user0['latitude'].max() > max_lat:\n",
    "        max_lat = user0['latitude'].max()\n",
    "    if user0['longitude'].min() < min_lon:\n",
    "        min_lon = user0['longitude'].min()\n",
    "    if user0['longitude'].max() > max_lon:\n",
    "        max_lon = user0['longitude'].max()\n",
    "\n",
    "print(f\"min_lat: {min_lat}, min_lon: {min_lon}\")\n",
    "print(f\"max_lat: {max_lat}, max_lon: {max_lon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valid_minmax_location import get_minmax_location\n",
    "\n",
    "get_minmax_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valid_minmax_location import LocationPreprocessor\n",
    "\n",
    "locationPreprocess = LocationPreprocessor()\n",
    "center_locatuon = locationPreprocess.get_center_location()\n",
    "\n",
    "earth_radius = 6371000\n",
    "\n",
    "def convert_coord_for_blender(lat, lon):\n",
    "    delta_lat = lat - center_locatuon[0]\n",
    "    delta_lon = lon - center_locatuon[1]\n",
    "    \n",
    "    x = delta_lon * earth_radius * (np.pi / 180) * np.cos(lat * (np.pi / 180))\n",
    "    y = delta_lat * earth_radius * (np.pi / 180)\n",
    " \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_coord_for_blender(self, lat, lon):\n",
    "    delta_lat = lat - self.center_coord[0]\n",
    "    delta_lon = lon - self.center_coord[1]\n",
    "\n",
    "    x = delta_lon * self.earth_radius * (np.pi / 180) * np.cos(lat * (np.pi / 180))\n",
    "    y = delta_lat * self.earth_radius * (np.pi / 180)\n",
    " \n",
    "    return x, y\n",
    " \n",
    "earth_radius = 6371000\n",
    "lower_left_coord = [self.config['coords']['lower_left']['lat'], self.config['coords']['lower_left']['lon']]\n",
    "upper_right_coord = [self.config['coords']['upper_right']['lat'], self.config['coords']['upper_right']['lon']]\n",
    "center_coord = [\n",
    "    (self.lower_left_coord[0] + self.upper_right_coord[0]) / 2,\n",
    "    (self.lower_left_coord[1] + self.upper_right_coord[1]) / 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(0).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset   (label) , \n",
    "# DataLoader  Dataset         (iterable) .\n",
    "# https://wikidocs.net/156998\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "    # ,   \n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 99, 95]]\n",
    "        self.y_data = [[152], \n",
    "                       [185]]\n",
    "    \n",
    "    def __len__(self):\n",
    "    #     \n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    # idx     .\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    \n",
    "customData = CustomDataset()\n",
    "customData.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "index = 10\n",
    "data_dir = 'Data/'\n",
    "csv_dir = 'csv/'\n",
    "csv_extension = '.csv'\n",
    "user_path_list = os.listdir(data_dir)\n",
    "csv_path = os.path.join(data_dir, user_path_list[index], csv_dir)\n",
    "user_file = csv_path + user_path_list[index] + '.csv'\n",
    "df = pd.read_csv(user_file)\n",
    "df[[\"days\",\"latitude\", \"longitude\"]].head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Data folder    User folder \n",
    "#  User data train-test   \n",
    "# train_set(dataset), test_set(dataset)   \n",
    "\n",
    "class GeoLifeDataSet(Dataset):\n",
    "    def __init__(self, data_dir, user_list, samples_s, samples_q, length, y_timestep):\n",
    "        self.data_dir   = data_dir\n",
    "        self.csv_dir    = 'csv/'\n",
    "        self.user_list  = user_list\n",
    "        # user_list: all user\n",
    "        self.samples_s  = samples_s\n",
    "        # samples_s: the number of support set\n",
    "        self.samples_q  = samples_q\n",
    "        # samples_q: the number of query set\n",
    "        self.length     = length \n",
    "        # length: the length of mini batch of a user\n",
    "        self.y_timestep = y_timestep\n",
    "        # y_time_step: the next time step to be predicted\n",
    "        #              it must be less than length\n",
    "    \n",
    "    def sampleTime(self, dataset):\n",
    "        cur_ds = dataset.copy()\n",
    "        minibatch = []\n",
    "        \n",
    "        max_len = len(cur_ds)\n",
    "        ###############################################\n",
    "        # MAke sure samples from query and support \n",
    "        # do not intersect\n",
    "        ##############################################\n",
    "        # total_data_slice -> lenght     slice \n",
    "        total_data_slice = list(range(int(max_len/self.length)))\n",
    "        total_samps = self.samples_q + self.samples_s\n",
    "        \n",
    "        slice_point = int(len(total_data_slice)*(self.samples_s/total_samps))\n",
    "        # print(f\"slice_point: {slice_point}\")\n",
    "\n",
    "        s_s_list = total_data_slice[:slice_point]\n",
    "        q_s_list = total_data_slice[slice_point:]\n",
    "\n",
    "        replace = False\n",
    "        if total_samps > len(total_data_slice):\n",
    "            replace = True\n",
    "\n",
    "        s_s_list = np.random.choice(s_s_list, size=self.samples_s, replace=replace)\n",
    "        q_s_list = np.random.choice(q_s_list, size=self.samples_q, replace=replace)\n",
    "        \n",
    "        # print(f\"s_list:{s_s_list}\")\n",
    "        # print(f\"q_list:{q_s_list}\")\n",
    "        choice_list = np.concatenate([s_s_list, q_s_list])\n",
    "        # #################################################\n",
    "        # print(f\"choice_list: {choice_list}\")\n",
    "        \n",
    "        for idx in choice_list:\n",
    "            start_idx = idx * self.length\n",
    "            if max_len - self.length >= 0:\n",
    "                cur_sample = cur_ds.iloc[start_idx:(start_idx + self.length), :]\n",
    "                minibatch.append(cur_sample)\n",
    "            else:\n",
    "                fill_quota  = np.abs(self.length - max_len)\n",
    "                zeros_r     = np.zeros([fill_quota, cur_ds.shape[1]])\n",
    "                cur_sample  = cur_ds[:, :]\n",
    "                cur_sample  = np.concatenate([zeros_r, cur_sample], axis = 0)\n",
    "                minibatch.append(cur_sample)\n",
    "        return np.array(minibatch)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        csv_path = os.path.join(self.data_dir, self.user_list[index], self.csv_dir)\n",
    "        user_file = csv_path + self.user_list[index] + '.csv'\n",
    "        df = pd.read_csv(user_file)\n",
    "        df = df[['days','latitude', 'longitude']]\n",
    "\n",
    "        samples = self.sampleTime(df)\n",
    "        # print(f\"mini_batch: {samples.shape}\")\n",
    "        # mini_batch: (5, 10, 3)\n",
    "        \n",
    "        sup_x = np.array(samples[:self.samples_s, :-self.y_timestep, :])\n",
    "        sup_y = np.array(samples[:self.samples_s, -self.y_timestep:, -2:])\n",
    "        que_x = np.array(samples[self.samples_s:, :-self.y_timestep, :])\n",
    "        que_y = np.array(samples[self.samples_s:, -self.y_timestep:, -2:])\n",
    "\n",
    "        return (que_x, sup_x, sup_y), que_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        # batch     \n",
    "        #   batch   .\n",
    "        #   user    \n",
    "        return len(self.user_list)\n",
    "\n",
    "user_list = os.listdir(data_dir)\n",
    "random.shuffle(user_list)\n",
    "train_size = 0.1\n",
    "train_list = user_list[:(int)(len(user_list)*train_size)]\n",
    "print(f\"train_list: {len(train_list)}\")\n",
    "\n",
    "# dataset = GeoLifeDataSet(\"Data/\", [0, 1, 2, 3], 5, 2, 100, 10)\n",
    "# dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"Data/\"#\"data/geolife/Data/\"\n",
    "sample_s = 5\n",
    "sample_q = 3\n",
    "length = 100\n",
    "y_timestep = 10\n",
    "\n",
    "user_list = os.listdir(data_dir)\n",
    "random.shuffle(user_list)\n",
    "train_size = 0.1\n",
    "train_list = user_list[:(int)(len(user_list)*train_size)]\n",
    "test_list  = user_list[(int)(len(user_list)*train_size):]\n",
    "print(f\"train_list: {len(train_list)}\")\n",
    "\n",
    "training_data = GeoLifeDataSet(data_dir, train_list, sample_s, sample_q, length, y_timestep)\n",
    "test_data = GeoLifeDataSet(data_dir, train_list, sample_s, sample_q, length, y_timestep)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "train_x, train_y = next(iter(train_dataloader))\n",
    "print(f\"support_x: {train_x[0].shape}\")\n",
    "print(f\"support_y: {train_x[1].shape}\")\n",
    "print(f\"query_x: {train_x[2].shape}\")\n",
    "print(f\"query_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [1, 2, 3, 4, 5]\n",
    "shape[:-2] + [-1] + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # input is TASKS x SAMPLES x FEATURES x TIME x Latent vector\n",
    "        shape = torch._shape_as_tensor(inp)\n",
    "        # (3, 20, 6, 100, 1)\n",
    "        x = torch.reshape(inp, [-1, shape[-2], shape[-1]])\n",
    "        # (300, 100, 1)\n",
    "        x, f = self.gru(x)\n",
    "        # x:(300, 100, 32)\n",
    "        # f:(3, 100, 32)\n",
    "        \n",
    "        if self.final:\n",
    "            new_shape = shape[:-2].tolist() + [-1]\n",
    "            out = torch.reshape(f, new_shape)\n",
    "        else:\n",
    "            new_shape = shape[:-1].tolist() + [-1]\n",
    "            # (3, 20, 6, 100, -1)\n",
    "            out = torch.reshape(x, new_shape)\n",
    "            # (3, 20, 6, 100, 32)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('Data/000/csv/000.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('Data/001/csv/001.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = pd.read_csv('Data/000/csv/000.csv')\n",
    "df.head(1)\n",
    "df_temp = df[['latitude', 'longitude']].copy()\n",
    "\n",
    "model = KMeans(n_clusters=100, random_state=123)\n",
    "model.fit(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['label'] = model.labels_\n",
    "df_temp['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
