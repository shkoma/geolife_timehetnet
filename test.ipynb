{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True, device: cuda:0\n",
      "target latitude, longitude: tensor([[[[  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  57.,  101.],\n",
      "          [  58.,  101.]],\n",
      "\n",
      "         [[  62.,   81.],\n",
      "          [  62.,   81.],\n",
      "          [  62.,   81.],\n",
      "          [  62.,   81.],\n",
      "          [  62.,   81.],\n",
      "          [  63.,   81.],\n",
      "          [  63.,   81.],\n",
      "          [  63.,   81.],\n",
      "          [  63.,   81.],\n",
      "          [  63.,   81.],\n",
      "          [  64.,   81.],\n",
      "          [  64.,   81.]],\n",
      "\n",
      "         [[ 883., 2784.],\n",
      "          [ 883., 2784.],\n",
      "          [ 883., 2784.],\n",
      "          [ 883., 2784.],\n",
      "          [ 883., 2784.],\n",
      "          [ 883., 2784.],\n",
      "          [ 882., 2784.],\n",
      "          [ 882., 2784.],\n",
      "          [ 882., 2784.],\n",
      "          [ 882., 2784.],\n",
      "          [ 882., 2784.],\n",
      "          [ 882., 2784.]],\n",
      "\n",
      "         [[ 122.,  115.],\n",
      "          [ 122.,  116.],\n",
      "          [ 122.,  116.],\n",
      "          [ 122.,  117.],\n",
      "          [ 122.,  118.],\n",
      "          [ 121.,  118.],\n",
      "          [ 121.,  119.],\n",
      "          [ 121.,  119.],\n",
      "          [ 121.,  120.],\n",
      "          [ 121.,  121.],\n",
      "          [ 121.,  121.],\n",
      "          [ 121.,  122.]],\n",
      "\n",
      "         [[  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.]],\n",
      "\n",
      "         [[  13.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  15.,   69.],\n",
      "          [  15.,   69.]],\n",
      "\n",
      "         [[  13.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   68.],\n",
      "          [  14.,   68.],\n",
      "          [  14.,   68.],\n",
      "          [  14.,   68.],\n",
      "          [  14.,   68.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.],\n",
      "          [  14.,   69.]],\n",
      "\n",
      "         [[ 104.,  207.],\n",
      "          [ 104.,  207.],\n",
      "          [ 104.,  207.],\n",
      "          [ 104.,  207.],\n",
      "          [ 104.,  207.],\n",
      "          [ 104.,  207.],\n",
      "          [ 105.,  207.],\n",
      "          [ 105.,  207.],\n",
      "          [ 105.,  207.],\n",
      "          [ 105.,  207.],\n",
      "          [ 105.,  207.],\n",
      "          [ 105.,  207.]],\n",
      "\n",
      "         [[  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  88.,   74.],\n",
      "          [  88.,   74.],\n",
      "          [  88.,   74.],\n",
      "          [  87.,   74.],\n",
      "          [  87.,   74.]],\n",
      "\n",
      "         [[  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    5.],\n",
      "          [  36.,    4.]]]], device='cuda:0', dtype=torch.float64)\n",
      "Predicted latitude, longitude: tensor([[[[ -191.5436,  4344.8525],\n",
      "          [ 7693.0851,  9251.1544],\n",
      "          [ 1187.2414,  2192.9876],\n",
      "          [ 8635.1351,  2023.3011],\n",
      "          [-7966.9601, -4692.5414],\n",
      "          [ 2971.3470, -3850.5542],\n",
      "          [ 1883.5687,  1631.0504],\n",
      "          [ 5136.2519, -1166.8991],\n",
      "          [ 2078.2700,  -762.1954],\n",
      "          [-2614.6734,  3461.6001],\n",
      "          [  657.4309, -2755.0689],\n",
      "          [ 1865.5731,  -733.8252]],\n",
      "\n",
      "         [[  -81.5250,  4265.3636],\n",
      "          [ 7695.0737,  9166.8860],\n",
      "          [ 1127.7314,  2269.0698],\n",
      "          [ 8807.4889,  1875.7232],\n",
      "          [-7892.1838, -4552.3478],\n",
      "          [ 3058.0952, -3912.0274],\n",
      "          [ 1847.8277,  1670.6692],\n",
      "          [ 5160.8701, -1140.5869],\n",
      "          [ 2096.1544,  -631.9796],\n",
      "          [-2657.4919,  3378.2798],\n",
      "          [  715.3273, -2773.7305],\n",
      "          [ 1961.9176,  -687.2620]],\n",
      "\n",
      "         [[ 1552.1062,  3608.7085],\n",
      "          [ 7126.0068,  7332.8294],\n",
      "          [ 2472.7412,  2205.1892],\n",
      "          [ 8755.3985,  1845.4331],\n",
      "          [-4802.0004, -3737.2753],\n",
      "          [ 1891.8915, -3040.1735],\n",
      "          [ 2849.8470,  2879.9218],\n",
      "          [ 4871.8717,   -23.2593],\n",
      "          [ 2723.3308,  1067.5554],\n",
      "          [-1263.3152,  3444.6349],\n",
      "          [ 1383.9952, -3460.6540],\n",
      "          [ 1948.3085,   577.8368]],\n",
      "\n",
      "         [[ -137.4911,  4317.8280],\n",
      "          [ 7802.6187,  9225.9102],\n",
      "          [ 1212.8238,  2186.2979],\n",
      "          [ 8816.5423,  1892.4571],\n",
      "          [-7872.0593, -4613.3927],\n",
      "          [ 3147.4965, -3857.4211],\n",
      "          [ 1940.7267,  1677.9827],\n",
      "          [ 5088.4043, -1277.1931],\n",
      "          [ 2116.8357,  -711.7584],\n",
      "          [-2705.8731,  3434.8856],\n",
      "          [  754.9168, -2843.5308],\n",
      "          [ 2044.5602,  -576.3016]],\n",
      "\n",
      "         [[ -126.5520,  4249.4907],\n",
      "          [ 7602.8720,  9103.2721],\n",
      "          [ 1251.3078,  2088.1956],\n",
      "          [ 8710.3394,  1878.3131],\n",
      "          [-7692.5946, -4602.7912],\n",
      "          [ 3117.1685, -3742.5739],\n",
      "          [ 1939.2132,  1648.5410],\n",
      "          [ 4889.6959, -1324.3008],\n",
      "          [ 2076.7508,  -723.6944],\n",
      "          [-2646.8831,  3400.0029],\n",
      "          [  794.0164, -2804.0019],\n",
      "          [ 2118.8856,  -446.6413]],\n",
      "\n",
      "         [[ -160.1925,  4251.5778],\n",
      "          [ 7605.2500,  9071.6737],\n",
      "          [ 1235.5324,  2078.3806],\n",
      "          [ 8679.8742,  1836.1422],\n",
      "          [-7630.7679, -4573.4776],\n",
      "          [ 3125.6315, -3710.8665],\n",
      "          [ 1969.3799,  1667.1827],\n",
      "          [ 4856.5329, -1360.8807],\n",
      "          [ 2068.3595,  -754.1390],\n",
      "          [-2657.4322,  3405.7846],\n",
      "          [  786.7470, -2854.8869],\n",
      "          [ 2077.7111,  -424.5978]],\n",
      "\n",
      "         [[ -160.5934,  4250.8322],\n",
      "          [ 7607.2616,  9073.0858],\n",
      "          [ 1236.2725,  2079.1024],\n",
      "          [ 8678.8817,  1837.1593],\n",
      "          [-7636.4209, -4573.2092],\n",
      "          [ 3123.5772, -3713.5138],\n",
      "          [ 1969.5226,  1666.0877],\n",
      "          [ 4859.6533, -1360.4635],\n",
      "          [ 2067.7164,  -753.1303],\n",
      "          [-2656.9156,  3406.1496],\n",
      "          [  785.6896, -2852.5859],\n",
      "          [ 2078.4713,  -425.8014]],\n",
      "\n",
      "         [[  -22.1110,  4327.5576],\n",
      "          [ 7853.7238,  9280.6148],\n",
      "          [ 1285.1551,  2270.3576],\n",
      "          [ 8977.6332,  1925.0444],\n",
      "          [-7867.6179, -4627.1240],\n",
      "          [ 3099.1967, -3910.1792],\n",
      "          [ 1972.4876,  1762.5606],\n",
      "          [ 5236.2810, -1145.7477],\n",
      "          [ 2184.0733,  -588.0952],\n",
      "          [-2660.2569,  3461.4100],\n",
      "          [  788.2105, -2871.4753],\n",
      "          [ 2026.8631,  -600.8870]],\n",
      "\n",
      "         [[ -250.4485,  4382.0922],\n",
      "          [ 7747.4322,  9277.1894],\n",
      "          [ 1216.8070,  2137.9476],\n",
      "          [ 8644.4036,  2014.3940],\n",
      "          [-7927.9862, -4711.2556],\n",
      "          [ 3070.7370, -3794.3992],\n",
      "          [ 1931.3624,  1611.6455],\n",
      "          [ 5037.1987, -1337.2707],\n",
      "          [ 2099.6637,  -854.8177],\n",
      "          [-2675.8538,  3497.0449],\n",
      "          [  694.4301, -2810.7297],\n",
      "          [ 1918.3456,  -629.3618]],\n",
      "\n",
      "         [[ -190.7332,  4318.3274],\n",
      "          [ 7696.0682,  9234.0813],\n",
      "          [ 1160.6185,  2212.2759],\n",
      "          [ 8661.1730,  1975.2923],\n",
      "          [-7966.1468, -4649.5519],\n",
      "          [ 3014.3271, -3855.5867],\n",
      "          [ 1865.0029,  1615.3150],\n",
      "          [ 5124.2169, -1220.1947],\n",
      "          [ 2068.2123,  -761.8362],\n",
      "          [-2643.6588,  3439.7565],\n",
      "          [  661.2762, -2753.9814],\n",
      "          [ 1891.8624,  -707.3062]]]], device='cuda:0', dtype=torch.float64)\n",
      "loss: 5854.198937712711\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_mlp import MLP\n",
    "from torch_segment_dataset import SegmentDataset\n",
    "\n",
    "def metricDist(y_true, y_pred):\n",
    "    row = (y_pred[:,:,:,0] - y_true[:,:,:,0])**2\n",
    "    col = (y_pred[:,:,:,1] - y_true[:,:,:,1])**2\n",
    "    return torch.mean(row + col) ** 0.5\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(f\"use_cuda: {use_cuda}, device: {device}\")\n",
    "\n",
    "data_dir = \"data/geolife/Data/\"\n",
    "\n",
    "user_list = [\"068\", \"003\", \"004\"]\n",
    "min_length = 6\n",
    "time_delta = 60\n",
    "length = min_length * time_delta\n",
    "\n",
    "y_timestep = min_length * 2\n",
    "x_attribute = 20\n",
    "label_attribute = 2\n",
    "sample_s = 10\n",
    "replace = False\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# Dataset\n",
    "training_data    = SegmentDataset(data_dir, user_list, device, time_delta, y_timestep, length, label_attribute, sample_s, replace)\n",
    "train_dataloader = DataLoader(training_data, batch_size, shuffle=False)\n",
    "\n",
    "# Model 훈련 진행\n",
    "model = MLP(input_shape=[(length-y_timestep), x_attribute], y_timestep = y_timestep, label_attribute=label_attribute)\n",
    "\n",
    "model = model.to(torch.double)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for train_idx, train_data in enumerate(train_dataloader, 0):\n",
    "        task_X, task_y = train_data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(task_X)\n",
    "        loss = criterion(output, task_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 예측\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted = model(task_X)\n",
    "    loss = metricDist(task_y, predicted)\n",
    "    print(\"target latitude, longitude:\", task_y)\n",
    "    print(\"Predicted latitude, longitude:\", predicted)\n",
    "    print(f'loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "writer_dir_name = 'data/geolife/runs'\n",
    "log_dir_name = '[grid_origin_1000]_20240224-173505'\n",
    "log_folder = writer_dir_name + '/' + log_dir_name\n",
    "# best_model_path = log_folder + '/best_model.pth'\n",
    "best_model_path = log_folder + '/best_train_model.pth'\n",
    "\n",
    "config_df = pd.read_csv(log_folder + '/configuration.csv')\n",
    "config_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# writer_dir_name = 'data/geolife/runs'\n",
    "# log_dir_name = '[grid_origin_1000]_20240222-162606'\n",
    "# log_folder = writer_dir_name + '/' + log_dir_name\n",
    "# # best_model_path = log_folder + '/best_model.pth'\n",
    "# best_model_path = log_folder + '/best_train_model.pth'\n",
    "\n",
    "# config_df = pd.read_csv(log_folder + '/configuration.csv')\n",
    "\n",
    "data_dir = \"data/geolife/Data/\"\n",
    "label_attribute = 2\n",
    "\n",
    "sample_s = config_df['sample_s'][0]\n",
    "sample_q = config_df['sample_q'][0]\n",
    "\n",
    "args_epoch = config_df['epoch'][0]\n",
    "args_patience = config_df['patience'][0]\n",
    "\n",
    "gap_min = 12 # 1 min\n",
    "gap = gap_min\n",
    "\n",
    "day = config_df['day'][0]\n",
    "week = config_df['week'][0]\n",
    "y_timestep = config_df['y_timestep'][0]   #12 # must be less than length, 12 = 1 hour, 12 * 24 = 288 -> 1day\n",
    "length = config_df['length'][0] # -> 4 weeks\n",
    "\n",
    "train_list = config_df['train_list'][0]\n",
    "validation_list = config_df['val_list'][0]\n",
    "test_list = config_df['test_list'][0]\n",
    "\n",
    "train_size = 0.4\n",
    "validation_size = 0.1\n",
    "batch_size = 1 # each user\n",
    "\n",
    "## Test Phase\n",
    "is_train = False\n",
    "\n",
    "print(f\"y_timestep: {y_timestep}, length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 설정\n",
    "import ast\n",
    "from torch_time_het import TimeHetNet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geolife_dateset import GeoLifeDataSet\n",
    "from data.geolife.convert_minmax_location import LocationPreprocessor\n",
    "import random\n",
    "\n",
    "data_dir = \"data/geolife/Data/\"\n",
    "\n",
    "\n",
    "train_list = config_df['train_list'][0]\n",
    "validation_list = config_df['val_list'][0]\n",
    "test_list = config_df['val_list'][0]#, '078']\n",
    "\n",
    "test_list = ['068', '038']#, '039']#, '067']\n",
    "\n",
    "print(f\"train_list:      {train_list}\")\n",
    "print(f\"validation_list: {validation_list}\")\n",
    "print(f\"test_list:       {test_list}\")\n",
    "\n",
    "# Multi user\n",
    "training_data   = GeoLifeDataSet(data_dir, train_list, sample_s, sample_q, length, y_timestep, gap, label_attribute)\n",
    "validation_data = GeoLifeDataSet(data_dir, validation_list, sample_s, sample_q, length, y_timestep, gap, label_attribute)\n",
    "test_data       = GeoLifeDataSet(data_dir, test_list, sample_s, sample_q, length, y_timestep, gap, label_attribute)\n",
    "\n",
    "# single user\n",
    "# training_data   = GeoLifeDataSet(data_dir, train_list, sample_s, sample_q, length, y_timestep, gap, label_attribute, mode='train')\n",
    "# validation_data = GeoLifeDataSet(data_dir, validation_list, sample_s, sample_q, length, y_timestep, gap, label_attribute, mode='valid')\n",
    "# test_data       = GeoLifeDataSet(data_dir, test_list, sample_s, sample_q, length, y_timestep, gap, label_attribute, mode='valid')\n",
    "\n",
    "\n",
    "train_dataloader      = DataLoader(training_data, batch_size, shuffle=False)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size, shuffle=False)\n",
    "test_dataloader       = DataLoader(test_data, batch_size, shuffle=False)\n",
    "\n",
    "best_model = TimeHetNet(dims_inf = ast.literal_eval(\"[32,32,32]\"),\n",
    "                        dims_pred = ast.literal_eval(\"[32,32,32]\"), \n",
    "                        activation=\"relu\", \n",
    "                        time=100,\n",
    "                        batchnorm=False, \n",
    "                        block = str(\"gru,conv,conv,gru\").split(\",\"),\n",
    "                        output_shape=[y_timestep, 2],\n",
    "                        length=length)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jimmy-ai.tistory.com/30\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class_probs = []\n",
    "class_label = []\n",
    "que_x_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, test_data in enumerate(test_dataloader, 0):\n",
    "        test_X, test_y = test_data\n",
    "        if len(test_X) < 2:\n",
    "            continue\n",
    "        class_probs = []\n",
    "        class_label = []\n",
    "        que_x_list = []\n",
    "\n",
    "        que_x, _, _ = test_X\n",
    "        output = best_model(test_X)\n",
    "        class_probs.append(output.type(torch.int64))\n",
    "        class_label.append(test_y)\n",
    "        # print(que_x.shape)\n",
    "        que_x_list.append(que_x[:, :, -y_timestep:, 0])\n",
    "        # print(que_x[:, :, -y_timestep:, :])\n",
    "        \n",
    "        test_probs = torch.cat(class_probs)\n",
    "        test_label = torch.cat(class_label)\n",
    "        test_que_x = torch.cat(que_x_list)\n",
    "\n",
    "        # print(test_probs.shape)\n",
    "        # print(test_label.shape)\n",
    "        # print(test_que_x.shape)\n",
    "\n",
    "        for user_idx in range(test_label.shape[0]):\n",
    "            for test_idx in range(test_label.shape[1]):\n",
    "                df = pd.DataFrame(data=test_que_x[user_idx][0], columns=['Time'])\n",
    "                df_norm = MinMaxScaler().fit_transform(df)\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.title('Row graph')\n",
    "                # plt.scatter(df_norm, test_label[user_idx][test_idx][:, 0] , color='r', alpha=0.5)\n",
    "                # plt.scatter(df_norm, test_probs[user_idx][test_idx][:, 0], color='g', alpha=0.3)\n",
    "                plt.plot(df_norm, test_label[user_idx][test_idx][:, 0] , color='r', alpha=0.5)\n",
    "                plt.plot(df_norm, test_probs[user_idx][test_idx][:, 0], color='g', alpha=0.3)\n",
    "                plt.xlabel('time')\n",
    "                plt.ylabel('grid_row')\n",
    "                \n",
    "                # fig_col = plt.figure(figsize=(5, 3))\n",
    "                plt.subplot(2, 2, 2)\n",
    "                plt.title('Column graph')\n",
    "                # plt.scatter(df_norm, test_label[user_idx][test_idx][:, 1] , color='r', alpha=0.5)\n",
    "                # plt.scatter(df_norm, test_probs[user_idx][test_idx][:, 1], color='g', alpha=0.3)\n",
    "                plt.plot(df_norm, test_label[user_idx][test_idx][:, 1] , color='r', alpha=0.5)\n",
    "                plt.plot(df_norm, test_probs[user_idx][test_idx][:, 1], color='g', alpha=0.3)\n",
    "                plt.xlabel('time')\n",
    "                plt.ylabel('grid_col')\n",
    "                # plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jimmy-ai.tistory.com/30\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for idx in range(test_label.shape[0]):\n",
    "    df = pd.DataFrame(data=test_que_x[idx][0], columns=['Time'])\n",
    "    df_norm = MinMaxScaler().fit_transform(df)\n",
    "\n",
    "    fig_row = plt.figure(figsize=(9, 6))\n",
    "    ax_row = fig_row.add_subplot(111)\n",
    "    ax_row.plot(df_norm, test_label[idx][0][:, 0] , color='r', alpha=0.5)\n",
    "    ax_row.plot(df_norm, test_probs[idx][0][:, 0], color='g', alpha=0.3)\n",
    "    ax_row.set_xlabel('time')\n",
    "    ax_row.set_ylabel('grid_row')\n",
    "    \n",
    "    ax_col = fig_row.add_subplot(222)\n",
    "    ax_col.plot(df_norm, test_label[idx][0][:, 1] , color='r', alpha=0.5)\n",
    "    ax_col.plot(df_norm, test_probs[idx][0][:, 1], color='g', alpha=0.3)\n",
    "    ax_col.set_xlabel('time')\n",
    "    ax_col.set_ylabel('grid_col')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jimmy-ai.tistory.com/30\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# fig = plt.figure(figsize=(9, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# ax.scatter(x, y, z, color = 'r', alpha = 0.5)\n",
    "# ax.scatter(x, z, y, color = 'g', alpha = 0.5) # y와 z축 swap\n",
    "\n",
    "for idx in range(test_label.shape[0]):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.view_init(elev=10, azim=-50)\n",
    "    df = pd.DataFrame(data=test_que_x[idx][0], columns=['Time'])\n",
    "    df_norm = MinMaxScaler().fit_transform(df)\n",
    "    ax.scatter(test_label[idx][0][:, 0], test_label[idx][0][:, 1], df_norm,\n",
    "               color='r', alpha=0.5)\n",
    "    ax.scatter(test_probs[idx][0][:, 0], test_probs[idx][0][:, 1], df_norm, \n",
    "               color='g', alpha=0.5)\n",
    "    print(df_norm)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    0부터 9까지의 \"class_index\"를 가져온 후 해당 정밀도-재현율(precision-recall)\n",
    "    곡선을 그립니다\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# 모든 정밀도-재현율(precision-recall; pr) 곡선을 그립니다\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class My_Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, name=None):\n",
    "        super(My_Linear, self).__init__()\n",
    "        self.Linear = nn.Linear(in_features, out_features, bias, device)\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"My_Linear - X.shape: {x.shape}, name: {self.name}\")\n",
    "        x = self.Linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/geolife/Data/000/csv/000.csv')\n",
    "df.head(1)\n",
    "df_test = df.iloc[:, :-2].copy()\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['latitude', 'longitude', 'altitude', 'days']\n",
    "df_x = df_test[X].head(10).copy()\n",
    "df_x = df_to_tensor(df_x)\n",
    "print(df_x)\n",
    "df_x = df_x.unsqueeze(axis=-1)\n",
    "print(df_x)\n",
    "df_x = df_x.sum(axis=2)\n",
    "print(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df_test, train_size=0.7, random_state=123)\n",
    "\n",
    "X = ['latitude', 'longitude', 'altitude', 'days']\n",
    "y = ['latitude', 'longitude']\n",
    "\n",
    "row = 10\n",
    "train_set = train_set.iloc[:row, :].copy()\n",
    "\n",
    "train_X = df_to_tensor(train_set[X])\n",
    "train_y = df_to_tensor(train_set[y])\n",
    "\n",
    "model = My_Linear(4, 2)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "out_list = []\n",
    "loss_train = 0.0\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for input in train_X:\n",
    "        outputs = model(input)\n",
    "        print(outputs)\n",
    "        loss = loss_fn(outputs, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    \n",
    "    print(f\"epoch:{epoch}, loss:{loss_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
